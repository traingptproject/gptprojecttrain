{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dLNk GPT Agent v2 - Phase 2: Long-form Code Generation (v2.0 Optimized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\nüöÄ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î:\n- ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° 3-4 ‡πÄ‡∏ó‡πà‡∏≤ (~7-8 ‡∏ä‡∏°. ‡πÅ‡∏ó‡∏ô 29 ‡∏ä‡∏°.)\n- ‡πÉ‡∏ä‡πâ GPU ‡πÄ‡∏ï‡πá‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û (>80%)\n- Loss ‡∏•‡∏î‡∏•‡∏á‡∏à‡∏£‡∏¥‡∏á\n- ‡∏°‡∏µ Early Stopping\n- Real-time Logging ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô\n- Overfitting Prevention\n#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies...\")\nprint(\"‚è∞ ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3 ‡∏ô‡∏≤‡∏ó‡∏µ\")\n\n!pip install -q -U pip\n\n# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n!pip install -q -U \\\n    transformers \\\n    accelerate \\\n    peft \\\n    bitsandbytes \\\n    datasets \\\n    trl \\\n    huggingface_hub \\\n    wandb \\\n    requests \\\n    psutil \\\n    GPUtil\n\nprint(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"\")\nprint(\"‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Restart Runtime\")\nprint(\"   Runtime > Restart runtime\")\nprint(\"\")\nprint(\"‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å restart ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô Cell 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ (‡∏Ç‡πâ‡∏≤‡∏° Cell 1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 2: Configuration ‡πÅ‡∏•‡∏∞ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport json\nimport time\nimport requests\nfrom datetime import datetime\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\nfrom datasets import load_dataset, concatenate_datasets\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nimport psutil\ntry:\n    import GPUtil\n    GPU_AVAILABLE = True\nexcept:\n    GPU_AVAILABLE = False\n\nprint(\"‚úÖ Import libraries ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üîß CONFIGURATION - ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face Token\nHF_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å HF Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Telegram Bot Configuration\nTELEGRAM_BOT_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Bot Token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\nTELEGRAM_CHAT_ID = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Chat ID ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Weights & Biases (Optional - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á)\nWANDB_API_KEY = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å W&B API Key ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ\n\n# Model Configuration\nBASE_MODEL = \"dlnkgpt/dLNk-gpt-j-6b-agent-v1\"\nNEW_MODEL_NAME = \"dlnkgpt/dLNk-gpt-j-6b-agent-v2-phase2\"\n\n# Training Hyperparameters (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß!)\nMAX_SEQ_LENGTH = 1024  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 2048 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\nPER_DEVICE_TRAIN_BATCH_SIZE = 4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 1 ‚Üí 4\nGRADIENT_ACCUMULATION_STEPS = 8  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 4 ‚Üí 8\n# Effective batch size = 4 √ó 8 = 32\n\nLEARNING_RATE = 2e-4\nNUM_TRAIN_EPOCHS = 2  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 3 ‚Üí 2 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 24 ‡∏ä‡∏°.\nWARMUP_RATIO = 0.05\nWEIGHT_DECAY = 0.01\n\n# LoRA Configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.1\n\n# Logging & Checkpointing\nLOGGING_STEPS = 10\nSAVE_STEPS = 200  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 500 ‡πÄ‡∏û‡∏∑‡πà‡∏≠ checkpoint ‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\nEVAL_STEPS = 200\nTELEGRAM_NOTIFICATION_STEPS = 100  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 50 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î spam\n\n# W&B Configuration\nif WANDB_API_KEY:\n    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n    WANDB_PROJECT = \"dLNk-gpt-v2\"\n    WANDB_RUN_NAME = f\"phase2-optimized-{datetime.now().strftime('%Y%m%d-%H%M')}\"\nelse:\n    WANDB_PROJECT = None\n    WANDB_RUN_NAME = None\n\nprint(\"‚úÖ Configuration ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üì¶ Base Model: {BASE_MODEL}\")\nprint(f\"üéØ New Model: {NEW_MODEL_NAME}\")\nprint(f\"üìè Max Sequence Length: {MAX_SEQ_LENGTH}\")\nprint(f\"üìä Effective Batch Size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"üîÑ Epochs: {NUM_TRAIN_EPOCHS}\")\nprint(f\"‚è±Ô∏è ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 3: Telegram Notification Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_telegram(message):\n    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡πà‡∏≤‡∏ô Telegram\"\"\"\n    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n        return\n    \n    try:\n        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n        data = {\n            \"chat_id\": TELEGRAM_CHAT_ID,\n            \"text\": message,\n            \"parse_mode\": \"HTML\"\n        }\n        response = requests.post(url, data=data, timeout=10)\n        if response.status_code == 200:\n            print(\"‚úÖ Telegram notification sent\")\n        else:\n            print(f\"‚ö†Ô∏è Telegram notification failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Telegram error: {e}\")\n\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram\nsend_telegram(f\"üöÄ <b>‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2 (v2.0 Optimized)</b>\\\\n\\\\n‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\nüìä Effective Batch Size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\\\\n‚è±Ô∏è ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")\n\nprint(\"‚úÖ Telegram notification function ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 4: GPU Check ‡πÅ‡∏•‡∏∞ System Info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üñ•Ô∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö...\")\nprint(\"=\" * 60)\n\n# CPU Info\nprint(f\"üíª CPU: {psutil.cpu_count()} cores\")\nprint(f\"üß† RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n\n# GPU Info\nif torch.cuda.is_available():\n    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"üíæ VRAM Total: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n    print(f\"üíæ VRAM Available: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / (1024**3):.1f} GB\")\n    \n    if GPU_AVAILABLE:\n        gpus = GPUtil.getGPUs()\n        if gpus:\n            gpu = gpus[0]\n            print(f\"üî• GPU Load: {gpu.load * 100:.1f}%\")\n            print(f\"‚ö° GPU Memory Used: {gpu.memoryUsed} MB / {gpu.memoryTotal} MB ({gpu.memoryUtil * 100:.1f}%)\")\nelse:\n    print(\"‚ùå GPU ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n    raise RuntimeError(\"‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ GPU ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô!\")\n\nprint(\"=\" * 60)\nprint(\"‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 5: ‡πÇ‡∏´‡∏•‡∏î Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î base model...\")\nprint(f\"Model: {BASE_MODEL}\")\n\n# BitsAndBytesConfig ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# ‡πÇ‡∏´‡∏•‡∏î tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL,\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ‡πÇ‡∏´‡∏•‡∏î base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\n\n# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö k-bit training\nbase_model = prepare_model_for_kbit_training(base_model)\n\nprint(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î base model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\nprint(f\"üìä Model parameters: {base_model.num_parameters() / 1e9:.2f}B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 6: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA...\")\n\n# LoRA Configuration\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\"],\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# ‡πÄ‡∏û‡∏¥‡πà‡∏° LoRA adapters\nmodel = get_peft_model(base_model, lora_config)\n\n# ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô parameters ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\nprint(f\"üéØ Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\nprint(f\"üìä Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 7: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î training data...\")\n\n# ‡πÇ‡∏´‡∏•‡∏î datasets\nprint(\"  - CodeAlpaca-20k...\")\ndataset1 = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n\nprint(\"  - Python Code Instructions 18k...\")\ndataset2 = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=\"train\")\n\nprint(\"  - Code Instructions 120k (subset)...\")\ndataset3 = load_dataset(\"iamtarun/code_instructions_120k_alpaca\", split=\"train[:20000]\")\n\n# ‡∏£‡∏ß‡∏° datasets\ncombined_dataset = concatenate_datasets([dataset1, dataset2, dataset3])\n\nprint(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î training data ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(combined_dataset):,} examples\")\n\n# ‡πÅ‡∏ö‡πà‡∏á train/eval\nsplit_dataset = combined_dataset.train_test_split(test_size=0.05, seed=42)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\nprint(f\"üìä Train: {len(train_dataset):,} examples\")\nprint(f\"üìä Eval: {len(eval_dataset):,} examples\")\n\n# Formatting function\ndef format_instruction(example):\n    \"\"\"‡πÅ‡∏õ‡∏•‡∏á dataset ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\"\"\"\n    if \"instruction\" in example and \"output\" in example:\n        instruction = example.get(\"instruction\", \"\")\n        input_text = example.get(\"input\", \"\")\n        output = example.get(\"output\", \"\")\n        \n        if input_text:\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Input:\\\\n{input_text}\\\\n\\\\n### Response:\\\\n{output}\"\n        else:\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n{output}\"\n        \n        return {\"text\": text}\n    return {\"text\": \"\"}\n\n# Format datasets\ntrain_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\neval_dataset = eval_dataset.map(format_instruction, remove_columns=eval_dataset.column_names)\n\n# ‡πÅ‡∏™‡∏î‡∏á sample\nprint(\"\\\\nüìù Sample training example:\")\nprint(\"=\" * 60)\nprint(train_dataset[0][\"text\"][:500] + \"...\")\nprint(\"=\" * 60)\n\nprint(\"‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° training data ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 8: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ training arguments...\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./dLNk-gpt-v2-phase2-optimized\",\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=WEIGHT_DECAY,\n    logging_steps=LOGGING_STEPS,\n    eval_strategy=\"steps\",\n    eval_steps=EVAL_STEPS,\n    save_strategy=\"steps\",\n    save_steps=SAVE_STEPS,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n    report_to=\"wandb\" if WANDB_API_KEY else \"none\",\n    run_name=WANDB_RUN_NAME if WANDB_API_KEY else None,\n    logging_dir=\"./logs\",\n    logging_first_step=True,\n    disable_tqdm=False,\n)\n\nprint(\"‚úÖ Training arguments ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üìä Total training steps: ~{len(train_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS}\")\nprint(f\"‚è±Ô∏è ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 9: ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer ‡∏û‡∏£‡πâ‡∏≠‡∏° Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á trainer...\")\n\n# Custom Callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Telegram Notifications\nfrom transformers import TrainerCallback\n\nclass TelegramCallback(TrainerCallback):\n    def __init__(self, notification_steps=100):\n        self.notification_steps = notification_steps\n        self.start_time = time.time()\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if state.global_step % self.notification_steps == 0 and state.global_step > 0:\n            elapsed_time = time.time() - self.start_time\n            hours = int(elapsed_time // 3600)\n            minutes = int((elapsed_time % 3600) // 60)\n            \n            loss = logs.get(\"loss\", 0)\n            lr = logs.get(\"learning_rate\", 0)\n            \n            message = f\"üìä <b>Step {state.global_step}</b>\\\\n\\\\n\"\n            message += f\"üìâ Loss: {loss:.4f}\\\\n\"\n            message += f\"üìà LR: {lr:.2e}\\\\n\"\n            message += f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ: {hours}h {minutes}m\\\\n\"\n            message += f\"üîÑ Epoch: {state.epoch:.2f}/{NUM_TRAIN_EPOCHS}\"\n            \n            send_telegram(message)\n\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=lora_config,\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n    packing=False,\n    dataset_text_field=\"text\",\n    callbacks=[\n        TelegramCallback(notification_steps=TELEGRAM_NOTIFICATION_STEPS),\n        EarlyStoppingCallback(early_stopping_patience=3)\n    ]\n)\n\nprint(\"‚úÖ Trainer ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\nprint(\"‚úÖ Early Stopping: Enabled (patience=3)\")\nprint(\"‚úÖ Telegram Notifications: Enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 10: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\nprint(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2 (v2.0 Optimized)...\")\nprint(f\"‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 60)\n\nsend_telegram(f\"üöÄ <b>‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô!</b>\\\\n\\\\n‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô\ntrainer.train()\n\nprint(\"=\" * 60)\nprint(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\nprint(f\"‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\" * 60)\n\nsend_telegram(f\"‚úÖ <b>‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!</b>\\\\n\\\\n‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 11: Save ‡πÅ‡∏•‡∏∞ Upload ‡πÇ‡∏°‡πÄ‡∏î‡∏•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\nsend_telegram(\"üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\ntry:\n    # Save locally\n    trainer.model.save_pretrained(\"./final_model\")\n    tokenizer.save_pretrained(\"./final_model\")\n    print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n    \n    # Upload to Hugging Face\n    print(f\"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á {NEW_MODEL_NAME}...\")\n    trainer.model.push_to_hub(NEW_MODEL_NAME, token=HF_TOKEN)\n    tokenizer.push_to_hub(NEW_MODEL_NAME, token=HF_TOKEN)\n    \n    print(\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n    print(f\"üîó URL: https://huggingface.co/{NEW_MODEL_NAME}\")\n    \n    send_telegram(f\"‚úÖ <b>‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!</b>\\\\n\\\\nüîó <a href='https://huggingface.co/{NEW_MODEL_NAME}'>View Model</a>\")\n    \nexcept Exception as e:\n    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n    send_telegram(f\"‚ùå <b>‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î:</b>\\\\n\\\\n{str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cell 12: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\n# Test prompts\ntest_prompts = [\n    \"Write a Python function to implement binary search:\",\n    \"Explain SQL injection vulnerability and how to prevent it:\",\n    \"Create a simple web scraper using BeautifulSoup:\"\n]\n\nprint(\"=\" * 60)\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\\\nüß™ Test {i}: {prompt}\")\n    print(\"-\" * 60)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(response)\n    print(\"-\" * 60)\n\nprint(\"=\" * 60)\nprint(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\n\nsend_telegram(\"‚úÖ <b>‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!</b>\\\\n\\\\n‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô üéâ\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}