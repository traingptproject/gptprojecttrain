import json

notebook = {
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ dLNk GPT - Monitored Training with LINE Alerts\n",
                "\n",
                "## ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏û‡∏£‡πâ‡∏≠‡∏° Real-time Monitoring\n",
                "\n",
                "**‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå:**\n",
                "- ‚úÖ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô LINE ‡πÅ‡∏ö‡∏ö real-time (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)\n",
                "- ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç overfitting ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
                "- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö learning rate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
                "- ‚úÖ ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
                "- ‚úÖ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏∏‡∏Å epoch\n",
                "\n",
                "**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:**\n",
                "1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Runtime ‡πÄ‡∏õ‡πá‡∏ô GPU (T4 ‡∏´‡∏£‡∏∑‡∏≠ A100)\n",
                "2. ‡∏£‡∏±‡∏ô cells ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö\n",
                "3. ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô LINE\n",
                "4. ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏ß‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏π‡πÅ‡∏•‡πÄ‡∏≠‡∏á"
            ],
            "metadata": {"id": "intro"}
        },
        {
            "cell_type": "markdown",
            "source": ["## 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU"],
            "metadata": {"id": "gpu"}
        },
        {
            "cell_type": "code",
            "source": [
                "!nvidia-smi\n",
                "print(\"\\n‚úÖ GPU ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
            ],
            "metadata": {"id": "check_gpu"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": ["## 2Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Packages"],
            "metadata": {"id": "install"}
        },
        {
            "cell_type": "code",
            "source": [
                "%%capture\n",
                "!pip install -q transformers>=4.30.0 datasets>=2.12.0 accelerate>=0.20.0 peft>=0.4.0 bitsandbytes tensorboard\n",
                "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")"
            ],
            "metadata": {"id": "install_packages"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3Ô∏è‚É£ Login Hugging Face\n",
                "\n",
                "‡πÉ‡∏™‡πà Hugging Face Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á:"
            ],
            "metadata": {"id": "login"}
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "HF_TOKEN = \"\"  # üëà ‡πÉ‡∏™‡πà token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
                "\n",
                "if not HF_TOKEN:\n",
                "    print(\"‚ö†Ô∏è  ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Hugging Face token\")\n",
                "else:\n",
                "    login(token=HF_TOKEN)\n",
                "    print(\"‚úÖ Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
            ],
            "metadata": {"id": "hf_login"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": ["## 4Ô∏è‚É£ Clone Repository"],
            "metadata": {"id": "clone"}
        },
        {
            "cell_type": "code",
            "source": [
                "!git clone https://github.com/traingptproject/gptprojecttrain.git\n",
                "%cd gptprojecttrain\n",
                "!ls -la\n",
                "print(\"\\n‚úÖ Clone ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
            ],
            "metadata": {"id": "clone_repo"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á manus-mcp-cli\n",
                "\n",
                "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á LINE notifications:"
            ],
            "metadata": {"id": "mcp"}
        },
        {
            "cell_type": "code",
            "source": [
                "# Install manus-mcp-cli (mock version for Colab)\n",
                "!mkdir -p /usr/local/bin\n",
                "\n",
                "# Create a mock manus-mcp-cli that prints instead of sending\n",
                "with open('/usr/local/bin/manus-mcp-cli', 'w') as f:\n",
                "    f.write('''#!/bin/bash\n",
                "echo \"[LINE] $@\"\n",
                "echo '{\"sentMessages\":[{\"id\":\"test\"}]}'\n",
                "''')\n",
                "\n",
                "!chmod +x /usr/local/bin/manus-mcp-cli\n",
                "print(\"‚úÖ MCP CLI installed (mock mode for Colab)\")\n",
                "print(\"‚ö†Ô∏è  LINE messages will be printed to console instead\")"
            ],
            "metadata": {"id": "install_mcp"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6Ô∏è‚É£ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô üöÄ\n",
                "\n",
                "**‚è∞ ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1-2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (2 epochs)**\n",
                "\n",
                "‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞:\n",
                "- ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ\n",
                "- ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ epoch\n",
                "- ‡∏õ‡∏£‡∏±‡∏ö learning rate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô\n",
                "- ‡∏´‡∏¢‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤ overfitting\n",
                "\n",
                "**‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏≠‡∏á**"
            ],
            "metadata": {"id": "train"}
        },
        {
            "cell_type": "code",
            "source": [
                "!python train_test_monitored.py"
            ],
            "metadata": {"id": "run_training"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": ["## 7Ô∏è‚É£ ‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"],
            "metadata": {"id": "results"}
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "\n",
                "# Load metrics\n",
                "with open('./training_output_test/metrics_history.json', 'r') as f:\n",
                "    metrics = json.load(f)\n",
                "\n",
                "print(f\"üìä Total training steps: {len(metrics)}\")\n",
                "print(f\"\\nüìà Final metrics:\")\n",
                "print(json.dumps(metrics[-1], indent=2))\n",
                "\n",
                "# Plot loss curve\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "losses = [m.get('loss', 0) for m in metrics if 'loss' in m]\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(losses)\n",
                "plt.title('Training Loss Over Time')\n",
                "plt.xlabel('Step')\n",
                "plt.ylabel('Loss')\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚úÖ Training completed successfully!\")"
            ],
            "metadata": {"id": "view_results"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 8Ô∏è‚É£ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•"],
            "metadata": {"id": "test"}
        },
        {
            "cell_type": "code",
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "import torch\n",
                "\n",
                "print(\"üì• Loading trained model...\")\n",
                "\n",
                "model_path = \"./training_output_test/final_model\"\n",
                "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "\n",
                "print(\"‚úÖ Model loaded!\\n\")\n",
                "\n",
                "def generate(prompt, max_tokens=200):\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=max_tokens,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        do_sample=True,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test\n",
                "test_prompt = \"Write a Python function to calculate fibonacci:\"\n",
                "print(f\"Prompt: {test_prompt}\")\n",
                "print(\"=\"*80)\n",
                "response = generate(test_prompt)\n",
                "print(response)"
            ],
            "metadata": {"id": "test_model"},
            "execution_count": None,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ\n",
                "\n",
                "**‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**\n",
                "\n",
                "‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡πÄ‡∏ó‡∏£‡∏ô 2 epochs\n",
                "‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö monitoring ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ\n",
                "‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
                "\n",
                "**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:**\n",
                "1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö metrics ‡πÅ‡∏•‡∏∞ QA results\n",
                "2. ‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô full training (3 epochs, 54,000 samples)\n",
                "3. Deploy ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub\n",
                "\n",
                "**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Full Training:**\n",
                "‡πÉ‡∏ä‡πâ `AutoTrain_GPU_Colab_Enhanced.ipynb` ‡πÅ‡∏ó‡∏ô"
            ],
            "metadata": {"id": "summary"}
        }
    ]
}

with open('/home/ubuntu/gptprojecttrain/Monitored_Training_Colab.ipynb', 'w') as f:
    json.dump(notebook, f, indent=2)

print("‚úÖ Monitored training notebook created!")
