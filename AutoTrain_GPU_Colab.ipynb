{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# dLNk GPT Uncensored - AutoTrain on GPU\n",
        "\n",
        "This notebook trains the dLNk GPT uncensored model using Hugging Face AutoTrain with GPU acceleration.\n",
        "\n",
        "**Requirements:**\n",
        "- GPU Runtime (T4, A100, or V100)\n",
        "- Hugging Face Token\n",
        "- 12-16 hours training time\n",
        "\n",
        "**Steps:**\n",
        "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "2. Run all cells in order\n",
        "3. Monitor training progress"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AutoTrain and dependencies\n",
        "!pip install -q autotrain-advanced\n",
        "print(\"‚úì AutoTrain installed\")"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Login to Hugging Face"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Enter your Hugging Face token here\n",
        "HF_TOKEN = \"\"  # Paste your token between the quotes\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"‚ö†Ô∏è  Please enter your Hugging Face token above\")\n",
        "else:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"‚úì Logged in to Hugging Face\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "load_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"dlnkgpt/dlnkgpt-uncensored-dataset\")\n",
        "\n",
        "print(f\"\\n‚úì Dataset loaded\")\n",
        "print(f\"  Train: {len(dataset['train']):,} examples\")\n",
        "print(f\"  Validation: {len(dataset['validation']):,} examples\")\n",
        "\n",
        "# Check columns\n",
        "print(f\"\\nColumns: {dataset['train'].column_names}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample:\")\n",
        "print(dataset['train'][0]['text'][:200] + \"...\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to CSV format for AutoTrain\n",
        "import os\n",
        "\n",
        "print(\"Preparing dataset for AutoTrain...\")\n",
        "\n",
        "# Create directory\n",
        "os.makedirs(\"./autotrain_data\", exist_ok=True)\n",
        "\n",
        "# Convert train split to CSV\n",
        "train_df = pd.DataFrame(dataset['train'])\n",
        "train_df = train_df[['text']]  # Keep only text column\n",
        "train_df.to_csv(\"./autotrain_data/train.csv\", index=False)\n",
        "\n",
        "# Convert validation split to CSV\n",
        "valid_df = pd.DataFrame(dataset['validation'])\n",
        "valid_df = valid_df[['text']]  # Keep only text column\n",
        "valid_df.to_csv(\"./autotrain_data/valid.csv\", index=False)\n",
        "\n",
        "print(f\"\\n‚úì Dataset saved as CSV\")\n",
        "print(f\"  Train: {len(train_df):,} rows\")\n",
        "print(f\"  Valid: {len(valid_df):,} rows\")\n",
        "print(f\"  Location: ./autotrain_data/\")\n",
        "\n",
        "# Verify files\n",
        "!ls -lh ./autotrain_data/"
      ],
      "metadata": {
        "id": "prepare_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configure Training"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set HF token\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "\n",
        "# Training configuration\n",
        "config = {\n",
        "    'project_name': 'dlnkgpt-uncensored',\n",
        "    'model': 'EleutherAI/gpt-j-6b',\n",
        "    'data_path': './autotrain_data',\n",
        "    'text_column': 'text',\n",
        "    'epochs': 3,\n",
        "    'batch_size': 4,\n",
        "    'learning_rate': 2e-5,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'gradient_accumulation': 8,\n",
        "    'block_size': 512,\n",
        "    'lora_r': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'lora_dropout': 0.05\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Model: {config['model']}\")\n",
        "print(f\"  Epochs: {config['epochs']}\")\n",
        "print(f\"  Batch Size: {config['batch_size']}\")\n",
        "print(f\"  Effective Batch: {config['batch_size'] * config['gradient_accumulation']}\")\n",
        "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
        "print(f\"  LoRA: r={config['lora_r']}, alpha={config['lora_alpha']}\")\n",
        "print(f\"\\n‚è±Ô∏è  Estimated time: 12-16 hours on T4 GPU\")"
      ],
      "metadata": {
        "id": "configure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Start Training\n",
        "\n",
        "**Important:**\n",
        "- Training takes 12-16 hours on T4 GPU\n",
        "- You can close the browser and return later\n",
        "- Model will be pushed to Hugging Face Hub automatically"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start AutoTrain\n",
        "!autotrain llm \\\n",
        "  --train \\\n",
        "  --project-name {config['project_name']} \\\n",
        "  --model {config['model']} \\\n",
        "  --data-path {config['data_path']} \\\n",
        "  --text-column {config['text_column']} \\\n",
        "  --lr {config['learning_rate']} \\\n",
        "  --epochs {config['epochs']} \\\n",
        "  --batch-size {config['batch_size']} \\\n",
        "  --warmup-ratio {config['warmup_ratio']} \\\n",
        "  --gradient-accumulation {config['gradient_accumulation']} \\\n",
        "  --block_size {config['block_size']} \\\n",
        "  --logging-steps 100 \\\n",
        "  --eval-strategy epoch \\\n",
        "  --save-total-limit 2 \\\n",
        "  --peft \\\n",
        "  --lora-r {config['lora_r']} \\\n",
        "  --lora-alpha {config['lora_alpha']} \\\n",
        "  --lora-dropout {config['lora_dropout']} \\\n",        "  --quantization int8 \\
        "  --mixed-precision fp16 \\
\n",
        "  --push-to-hub \\\n",
        "  --username dlnkgpt \\\n",
        "  --token $HF_TOKEN"
      ],
      "metadata": {
        "id": "start_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Test Trained Model"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "print(\"Loading trained model...\")\n",
        "\n",
        "# Load base model\n",
        "print(\"  Loading GPT-J-6B...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"EleutherAI/gpt-j-6b\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "print(\"  Loading LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    \"dlnkgpt/dlnkgpt-uncensored\"\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"\\n‚úì Model loaded!\\n\")\n",
        "\n",
        "# Generation function\n",
        "def generate(prompt, max_length=200):\n",
        "    formatted = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "tests = [\n",
        "    \"Explain artificial intelligence\",\n",
        "    \"What is machine learning?\",\n",
        "    \"How do neural networks work?\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Testing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, prompt in enumerate(tests, 1):\n",
        "    print(f\"\\n[{i}] {prompt}\")\n",
        "    print(\"-\" * 70)\n",
        "    response = generate(prompt)\n",
        "    print(response)\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "### Training Complete! üéâ\n",
        "\n",
        "**Model Details:**\n",
        "- Base: GPT-J-6B (6B parameters)\n",
        "- Training: 54,000 examples\n",
        "- Validation: 6,000 examples\n",
        "- Method: LoRA/PEFT\n",
        "- Epochs: 3\n",
        "\n",
        "**Access Your Model:**\n",
        "- Hub: https://huggingface.co/dlnkgpt/dlnkgpt-uncensored\n",
        "- Local: ./dlnkgpt-uncensored/\n",
        "\n",
        "**Usage:**\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
        "model = PeftModel.from_pretrained(base, \"dlnkgpt/dlnkgpt-uncensored\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dlnkgpt/dlnkgpt-uncensored\")\n",
        "```"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
