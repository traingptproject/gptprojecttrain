{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ dLNk GPT Agent v2 - Phase 2 Training (v2.4 Final)\n\n**‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ó‡∏∏‡∏Å‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**\n- ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç NameError: os is not defined\n- ‚úÖ ‡∏£‡∏ß‡∏° imports ‡πÅ‡∏•‡∏∞ config ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n- ‚úÖ Cell structure ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n- ‚úÖ SFTTrainer ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß\n- ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á 100%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies...\")\nprint(\"‚è∞ ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 2-3 ‡∏ô‡∏≤‡∏ó‡∏µ\\n\")\n\n!pip install -q -U pip\n!pip install -q -U transformers accelerate peft bitsandbytes datasets trl huggingface_hub wandb requests psutil GPUtil\n\nprint(\"\\n‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"\\n‚ö†Ô∏è  ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Restart Runtime\")\nprint(\"   Runtime > Restart runtime\")\nprint(\"\\n‡∏´‡∏•‡∏±‡∏á restart ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô Cell 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ (‡∏Ç‡πâ‡∏≤‡∏° Cell 1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nimport json\nimport time\nimport requests\nfrom datetime import datetime\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    TrainingArguments, \n    EarlyStoppingCallback,\n    TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset, concatenate_datasets\nfrom trl import SFTTrainer\nimport psutil\n\ntry:\n    import GPUtil\n    GPU_AVAILABLE = True\nexcept:\n    GPU_AVAILABLE = False\n\nprint(\"‚úÖ Import libraries ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\n\")\n\n# ================================================================\n# üîß CONFIGURATION - ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n# ================================================================\n\n# Hugging Face Token\nHF_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å HF Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Telegram Bot Configuration\nTELEGRAM_BOT_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Bot Token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\nTELEGRAM_CHAT_ID = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Chat ID ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Weights & Biases (Optional)\nWANDB_API_KEY = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å W&B API Key ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ\n\n# Model Configuration\nBASE_MODEL = \"dlnkgpt/dLNk-gpt-j-6b-agent-v1\"\nNEW_MODEL_NAME = \"dlnkgpt/dLNk-gpt-j-6b-agent-v2-phase2\"\n\n# Training Hyperparameters\nMAX_SEQ_LENGTH = 1024\nPER_DEVICE_TRAIN_BATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 8\nLEARNING_RATE = 2e-4\nNUM_TRAIN_EPOCHS = 2\nWARMUP_RATIO = 0.05\nWEIGHT_DECAY = 0.01\n\n# LoRA Configuration\nLORA_R = 16\nLORA_ALPHA = 32\nLORA_DROPOUT = 0.1\n\n# Logging & Checkpointing\nLOGGING_STEPS = 10\nSAVE_STEPS = 200\nEVAL_STEPS = 200\nTELEGRAM_NOTIFICATION_STEPS = 100\n\n# W&B Configuration\nif WANDB_API_KEY:\n    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n    WANDB_PROJECT = \"dLNk-gpt-v2\"\n    WANDB_RUN_NAME = f\"phase2-v2.4-{datetime.now().strftime('%Y%m%d-%H%M')}\"\nelse:\n    WANDB_PROJECT = None\n    WANDB_RUN_NAME = None\n\nprint(\"=\"*60)\nprint(\"‚úÖ Configuration ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"=\"*60)\nprint(f\"üì¶ Base Model: {BASE_MODEL}\")\nprint(f\"üéØ New Model: {NEW_MODEL_NAME}\")\nprint(f\"üìè Max Seq Length: {MAX_SEQ_LENGTH}\")\nprint(f\"üìä Effective Batch Size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"üîÑ Epochs: {NUM_TRAIN_EPOCHS}\")\nprint(f\"‚è±Ô∏è  ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")\nprint(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def send_telegram(message):\n    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡πà‡∏≤‡∏ô Telegram\"\"\"\n    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n        print(\"‚ö†Ô∏è  Telegram not configured\")\n        return\n    \n    try:\n        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n        data = {\n            \"chat_id\": TELEGRAM_CHAT_ID,\n            \"text\": message,\n            \"parse_mode\": \"HTML\"\n        }\n        response = requests.post(url, data=data, timeout=10)\n        if response.status_code == 200:\n            print(\"‚úÖ Telegram notification sent\")\n        else:\n            print(f\"‚ö†Ô∏è  Telegram failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Telegram error: {e}\")\n\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram\nprint(\"\\nüîî ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram notification...\")\nsend_telegram(\n    f\"üöÄ <b>‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2 (v2.1 Fixed)</b>\\\\n\\\\n\"\n    f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\"\n    f\"üìä Batch Size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\\\\n\"\n    f\"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\"\n)\nprint(\"‚úÖ Telegram function ready\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üñ•Ô∏è  ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö\")\nprint(\"=\"*60)\n\n# CPU & RAM\nprint(f\"üíª CPU: {psutil.cpu_count()} cores\")\nprint(f\"üß† RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n\n# GPU\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    gpu_free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / (1024**3)\n    \n    print(f\"üéÆ GPU: {gpu_name}\")\n    print(f\"üíæ VRAM Total: {gpu_total:.1f} GB\")\n    print(f\"üíæ VRAM Free: {gpu_free:.1f} GB\")\n    \n    if GPU_AVAILABLE:\n        try:\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu = gpus[0]\n                print(f\"üî• GPU Load: {gpu.load * 100:.1f}%\")\n                print(f\"‚ö° GPU Memory: {gpu.memoryUsed} MB / {gpu.memoryTotal} MB\")\n        except:\n            pass\nelse:\n    print(\"‚ùå GPU ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n    raise RuntimeError(\"‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ GPU ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô!\")\n\nprint(\"=\"*60)\nprint(\"‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\nprint(\"=\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î base model...\")\nprint(f\"Model: {BASE_MODEL}\")\nprint(\"=\"*60)\n\n# BitsAndBytesConfig\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL,\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\nprint(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î tokenizer ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n\nprint(\"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î model (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 2-3 ‡∏ô‡∏≤‡∏ó‡∏µ)...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\nprint(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n\nprint(\"‚è≥ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training...\")\nbase_model = prepare_model_for_kbit_training(base_model)\nprint(\"‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î base model ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üìä Model parameters: {base_model.num_parameters() / 1e9:.2f}B\")\nprint(\"=\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA...\")\nprint(\"=\"*60)\n\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\"],\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nprint(\"‚è≥ ‡πÄ‡∏û‡∏¥‡πà‡∏° LoRA adapters...\")\nmodel = get_peft_model(base_model, lora_config)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\nprint(\"=\"*60)\nprint(f\"üéØ Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\nprint(f\"üìä Total params: {total_params:,}\")\nprint(\"=\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î training data...\")\nprint(\"=\"*60)\n\nprint(\"‚è≥ ‡πÇ‡∏´‡∏•‡∏î CodeAlpaca-20k...\")\ndataset1 = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\nprint(f\"   ‚úÖ {len(dataset1):,} examples\")\n\nprint(\"‚è≥ ‡πÇ‡∏´‡∏•‡∏î Python Code Instructions 18k...\")\ndataset2 = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=\"train\")\nprint(f\"   ‚úÖ {len(dataset2):,} examples\")\n\nprint(\"‚è≥ ‡πÇ‡∏´‡∏•‡∏î Code Instructions 120k (subset)...\")\ndataset3 = load_dataset(\"iamtarun/code_instructions_120k_alpaca\", split=\"train[:20000]\")\nprint(f\"   ‚úÖ {len(dataset3):,} examples\")\n\nprint(\"\\n‚è≥ ‡∏£‡∏ß‡∏° datasets...\")\ncombined_dataset = concatenate_datasets([dataset1, dataset2, dataset3])\nprint(f\"‚úÖ ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(combined_dataset):,} examples\")\n\nprint(\"\\n‚è≥ ‡πÅ‡∏ö‡πà‡∏á train/eval...\")\nsplit_dataset = combined_dataset.train_test_split(test_size=0.05, seed=42)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î data ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"=\"*60)\nprint(f\"üìä Train: {len(train_dataset):,} examples\")\nprint(f\"üìä Eval: {len(eval_dataset):,} examples\")\nprint(\"=\"*60)\n\n# Format function\ndef format_instruction(example):\n    if \"instruction\" in example and \"output\" in example:\n        instruction = example.get(\"instruction\", \"\")\n        input_text = example.get(\"input\", \"\")\n        output = example.get(\"output\", \"\")\n        \n        if input_text:\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Input:\\\\n{input_text}\\\\n\\\\n### Response:\\\\n{output}\"\n        else:\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n{output}\"\n        \n        return {\"text\": text}\n    return {\"text\": \"\"}\n\nprint(\"\\n‚è≥ Format datasets...\")\ntrain_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\neval_dataset = eval_dataset.map(format_instruction, remove_columns=eval_dataset.column_names)\nprint(\"‚úÖ Format ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\\n\")\n\n# Show sample\nprint(\"üìù Sample training example:\")\nprint(\"-\"*60)\nprint(train_dataset[0][\"text\"][:300] + \"...\")\nprint(\"-\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ training arguments...\")\nprint(\"=\"*60)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./dLNk-gpt-v2-phase2-v2.1\",\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=WARMUP_RATIO,\n    weight_decay=WEIGHT_DECAY,\n    logging_steps=LOGGING_STEPS,\n    eval_strategy=\"steps\",\n    eval_steps=EVAL_STEPS,\n    save_strategy=\"steps\",\n    save_steps=SAVE_STEPS,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n    report_to=\"wandb\" if WANDB_API_KEY else \"none\",\n    run_name=WANDB_RUN_NAME if WANDB_API_KEY else None,\n    logging_dir=\"./logs\",\n    logging_first_step=True,\n    disable_tqdm=False,\n)\n\ntotal_steps = len(train_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Training arguments ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"=\"*60)\nprint(f\"üìä Total steps: ~{total_steps}\")\nprint(f\"üíæ Checkpoint every: {SAVE_STEPS} steps\")\nprint(f\"üìä Eval every: {EVAL_STEPS} steps\")\nprint(f\"‚è±Ô∏è  ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")\nprint(\"=\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üîß ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á trainer...\")\nprint(\"=\"*60)\n\n# Custom Callback\nclass DetailedLoggingCallback(TrainerCallback):\n    def __init__(self, notification_steps=100):\n        self.notification_steps = notification_steps\n        self.start_time = time.time()\n        self.last_log_time = time.time()\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        # Print detailed log every logging_steps\n        if logs:\n            current_time = time.time()\n            elapsed = current_time - self.start_time\n            since_last = current_time - self.last_log_time\n            self.last_log_time = current_time\n            \n            hours = int(elapsed // 3600)\n            minutes = int((elapsed % 3600) // 60)\n            \n            loss = logs.get(\"loss\", logs.get(\"eval_loss\", 0))\n            lr = logs.get(\"learning_rate\", 0)\n            \n            print(f\"\\n{'='*60}\")\n            print(f\"üìä Step {state.global_step} | Epoch {state.epoch:.2f}/{NUM_TRAIN_EPOCHS}\")\n            print(f\"{'='*60}\")\n            print(f\"üìâ Loss: {loss:.4f}\")\n            print(f\"üìà Learning Rate: {lr:.2e}\")\n            print(f\"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô: {hours}h {minutes}m\")\n            print(f\"‚ö° ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß: {LOGGING_STEPS/since_last:.2f} it/s\")\n            print(f\"{'='*60}\\n\")\n            \n            # Telegram notification\n            if state.global_step % self.notification_steps == 0 and state.global_step > 0:\n                message = (\n                    f\"üìä <b>Step {state.global_step}/{total_steps}</b>\\\\n\\\\n\"\n                    f\"üìâ Loss: {loss:.4f}\\\\n\"\n                    f\"üìà LR: {lr:.2e}\\\\n\"\n                    f\"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤: {hours}h {minutes}m\\\\n\"\n                    f\"üîÑ Epoch: {state.epoch:.2f}/{NUM_TRAIN_EPOCHS}\"\n                )\n                send_telegram(message)\n    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        print(f\"\\n{'='*60}\")\n        print(f\"‚úÖ Epoch {int(state.epoch)} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\n        print(f\"{'='*60}\\n\")\n\nprint(\"‚è≥ ‡∏™‡∏£‡πâ‡∏≤‡∏á SFTTrainer...\")\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    args=training_args,\n    packing=False,\n    dataset_text_field=\"text\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    callbacks=[\n        DetailedLoggingCallback(notification_steps=TELEGRAM_NOTIFICATION_STEPS),\n        EarlyStoppingCallback(early_stopping_patience=3)\n    ]\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Trainer ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\nprint(\"=\"*60)\nprint(\"‚úÖ Detailed Logging: Enabled\")\nprint(\"‚úÖ Early Stopping: Enabled (patience=3)\")\nprint(\"‚úÖ Telegram Notifications: Enabled\")\nprint(\"=\"*60)\nprint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\nprint(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2 (v2.1 Fixed)\")\nprint(\"=\"*60)\nprint(f\"‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"üìä Total steps: ~{total_steps}\")\nprint(f\"‚è±Ô∏è  ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤: ~7-8 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\")\nprint(\"=\"*60)\nprint(\"\\nüî• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô... (‡∏î‡∏π log ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)\\n\")\n\nsend_telegram(\n    f\"üöÄ <b>‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß!</b>\\\\n\\\\n\"\n    f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\"\n    f\"üìä Steps: {total_steps}\\\\n\"\n    f\"‚è±Ô∏è  ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: ~7-8 ‡∏ä‡∏°.\"\n)\n\n# Start training\ntrainer.train()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\nprint(\"=\"*60)\nprint(f\"‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"=\"*60)\nprint()\n\nsend_telegram(\n    f\"‚úÖ <b>‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!</b>\\\\n\\\\n\"\n    f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\nprint(\"=\"*60)\n\nsend_telegram(\"üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\ntry:\n    print(\"‚è≥ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á...\")\n    trainer.model.save_pretrained(\"./final_model\")\n    tokenizer.save_pretrained(\"./final_model\")\n    print(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n    \n    print(f\"\\n‚è≥ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á {NEW_MODEL_NAME}...\")\n    trainer.model.push_to_hub(NEW_MODEL_NAME, token=HF_TOKEN)\n    tokenizer.push_to_hub(NEW_MODEL_NAME, token=HF_TOKEN)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n    print(\"=\"*60)\n    print(f\"üîó URL: https://huggingface.co/{NEW_MODEL_NAME}\")\n    print(\"=\"*60)\n    print()\n    \n    send_telegram(\n        f\"‚úÖ <b>‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!</b>\\\\n\\\\n\"\n        f\"üîó <a href='https://huggingface.co/{NEW_MODEL_NAME}'>View Model</a>\"\n    )\n    \nexcept Exception as e:\n    print(f\"\\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}\")\n    send_telegram(f\"‚ùå <b>Error:</b>\\\\n\\\\n{str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\nprint(\"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\")\nprint(\"=\"*60)\n\ntest_prompts = [\n    \"Write a Python function to implement binary search:\",\n    \"Explain SQL injection and how to prevent it:\",\n    \"Create a web scraper using BeautifulSoup:\"\n]\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"üß™ Test {i}: {prompt}\")\n    print(\"-\"*60)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(response[:500])\n    print(\"-\"*60)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")\nprint(\"=\"*60)\n\nsend_telegram(\"‚úÖ <b>‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!</b>\\\\n\\\\n‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô üéâ\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}