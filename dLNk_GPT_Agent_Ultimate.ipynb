{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ dLNk GPT - Ultimate Agent Production System\\n",
        "\\n",
        "## ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£\\n",
        "\\n",
        "### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:\\n",
        "- üîê **Cybersecurity & Exploits** (2.2M+ rows)\\n",
        "- ü§ñ **Agent Capabilities** (4M+ rows)\\n",
        "- üõ†Ô∏è **Tool Use & Function Calling** (1.5M+ rows)\\n",
        "- üíª **Code Execution** \\n",
        "- üåê **Real-time Data Access**\\n",
        "- üß† **Self-correction & Reasoning**\\n",
        "\\n",
        "**‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 7.7M+ rows**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes wandb requests beautifulsoup4 tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\\n",
        "\\n",
        "# Hugging Face Token (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\\n",
        "HF_TOKEN = \\"YOUR_HF_TOKEN_HERE\\"\\n",
        "os.environ[\\"HF_TOKEN\\"] = HF_TOKEN\\n",
        "\\n",
        "# Telegram Bot (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\\n",
        "TELEGRAM_BOT_TOKEN = \\"8505747217:AAHpbWUfgrcQgJcK1JnqK9sds9nSwsTqVBg\\"\\n",
        "TELEGRAM_CHAT_ID = \\"7420166612\\"\\n",
        "\\n",
        "# W&B API Key (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)\\n",
        "WANDB_API_KEY = \\"\\"\\n",
        "if WANDB_API_KEY:\\n",
        "    os.environ[\\"WANDB_API_KEY\\"] = WANDB_API_KEY\\n",
        "    import wandb\\n",
        "    wandb.login()\\n",
        "\\n",
        "print(\\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Keys ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Telegram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\\n",
        "from datetime import datetime\\n",
        "\\n",
        "def send_telegram(message):\\n",
        "    \\"\\"\\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô Telegram\\"\\"\\"\\n",
        "    try:\\n",
        "        url = f\\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\\"\\n",
        "        data = {\\n",
        "            \\"chat_id\\": TELEGRAM_CHAT_ID,\\n",
        "            \\"text\\": f\\"ü§ñ dLNk GPT Agent\\\\n\\\\n{message}\\\\n\\\\n‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\",\\n",
        "            \\"parse_mode\\": \\"HTML\\"\\n",
        "        }\\n",
        "        response = requests.post(url, data=data)\\n",
        "        return response.json()\\n",
        "    except Exception as e:\\n",
        "        print(f\\"‚ö†Ô∏è Telegram error: {e}\\")\\n",
        "        return None\\n",
        "\\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\\n",
        "send_telegram(\\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô dLNk GPT Agent\\")\\n",
        "print(\\"‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cybersecurity (2.2M+ rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\\n",
        "from tqdm import tqdm\\n",
        "import json\\n",
        "\\n",
        "send_telegram(\\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Cybersecurity\\")\\n",
        "\\n",
        "cybersecurity_datasets = []\\n",
        "\\n",
        "# Top Cybersecurity Datasets\\n",
        "datasets_to_load = [\\n",
        "    \\"jason-oneal/mitre-stix-cve-exploitdb-dataset-alpaca\\",\\n",
        "    \\"clydeiii/cybersecurity\\",\\n",
        "    \\"Waiper/ExploitDB_DataSet\\",\\n",
        "    \\"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\\",\\n",
        "    \\"Vanessasml/cybersecurity_32k_instruction_input_output\\",\\n",
        "    \\"Nitral-AI/Cybersecurity-ShareGPT\\",\\n",
        "    \\"detoxioai/exploit_db_train_v1\\",\\n",
        "    \\"darkknight25/Reverse_Shell_Payloads_Dataset\\"\\n",
        "]\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Cybersecurity datasets...\\")\\n",
        "for dataset_name in tqdm(datasets_to_load):\\n",
        "    try:\\n",
        "        ds = load_dataset(dataset_name, split=\\"train\\", token=HF_TOKEN)\\n",
        "        cybersecurity_datasets.append(ds)\\n",
        "        print(f\\"   ‚úÖ {dataset_name}: {len(ds)} rows\\")\\n",
        "    except Exception as e:\\n",
        "        print(f\\"   ‚ö†Ô∏è {dataset_name}: {e}\\")\\n",
        "\\n",
        "print(f\\"\\\\n‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Cybersecurity datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(cybersecurity_datasets)} datasets\\")\\n",
        "send_telegram(f\\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Cybersecurity datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(cybersecurity_datasets)} datasets\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Agent (4M+ rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Agent\\")\\n",
        "\\n",
        "agent_datasets = []\\n",
        "\\n",
        "# Top Agent Datasets\\n",
        "agent_datasets_to_load = [\\n",
        "    \\"Agent-Ark/Toucan-1.5M\\",\\n",
        "    \\"neulab/agent-data-collection\\",\\n",
        "    \\"Kwai-Klear/SWE-smith-mini_swe_agent_plus-trajectories-66k\\",\\n",
        "    \\"Simia-Agent/Simia-Tau-SFT-90k-llama3json\\",\\n",
        "    \\"agentica-org/DeepScaleR-Preview-Dataset\\",\\n",
        "    \\"agentica-org/DeepCoder-Preview-Dataset\\",\\n",
        "    \\"AgentGym/AgentTraj-L\\",\\n",
        "    \\"zai-org/AgentInstruct\\",\\n",
        "    \\"antgroup/Agentar-DeepFinance-100K\\",\\n",
        "    \\"nebius/SWE-agent-trajectories\\"\\n",
        "]\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Agent datasets...\\")\\n",
        "for dataset_name in tqdm(agent_datasets_to_load):\\n",
        "    try:\\n",
        "        ds = load_dataset(dataset_name, split=\\"train\\", token=HF_TOKEN)\\n",
        "        agent_datasets.append(ds)\\n",
        "        print(f\\"   ‚úÖ {dataset_name}: {len(ds)} rows\\")\\n",
        "    except Exception as e:\\n",
        "        print(f\\"   ‚ö†Ô∏è {dataset_name}: {e}\\")\\n",
        "\\n",
        "print(f\\"\\\\n‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Agent datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(agent_datasets)} datasets\\")\\n",
        "send_telegram(f\\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Agent datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(agent_datasets)} datasets\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Function Calling (1.5M+ rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Function Calling\\")\\n",
        "\\n",
        "function_calling_datasets = []\\n",
        "\\n",
        "# Top Function Calling Datasets\\n",
        "function_datasets_to_load = [\\n",
        "    \\"HydraLM/glaive_function_calling_v1_standardized\\",\\n",
        "    \\"vietgpt/glaive-function-calling-v2\\",\\n",
        "    \\"glaiveai/glaive-function-calling-v2\\",\\n",
        "    \\"Locutusque/function-calling-chatml\\",\\n",
        "    \\"hypervariance/function-calling-sharegpt\\",\\n",
        "    \\"Salesforce/xlam-function-calling-60k\\",\\n",
        "    \\"NousResearch/hermes-function-calling-v1\\"\\n",
        "]\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Function Calling datasets...\\")\\n",
        "for dataset_name in tqdm(function_datasets_to_load):\\n",
        "    try:\\n",
        "        ds = load_dataset(dataset_name, split=\\"train\\", token=HF_TOKEN)\\n",
        "        function_calling_datasets.append(ds)\\n",
        "        print(f\\"   ‚úÖ {dataset_name}: {len(ds)} rows\\")\\n",
        "    except Exception as e:\\n",
        "        print(f\\"   ‚ö†Ô∏è {dataset_name}: {e}\\")\\n",
        "\\n",
        "print(f\\"\\\\n‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Function Calling datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(function_calling_datasets)} datasets\\")\\n",
        "send_telegram(f\\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Function Calling datasets ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(function_calling_datasets)} datasets\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\\")\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...\\")\\n",
        "\\n",
        "all_datasets = cybersecurity_datasets + agent_datasets + function_calling_datasets\\n",
        "\\n",
        "total_rows = sum([len(ds) for ds in all_datasets])\\n",
        "print(f\\"\\\\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\\")\\n",
        "print(f\\"   - Cybersecurity: {len(cybersecurity_datasets)} datasets\\")\\n",
        "print(f\\"   - Agent: {len(agent_datasets)} datasets\\")\\n",
        "print(f\\"   - Function Calling: {len(function_calling_datasets)} datasets\\")\\n",
        "print(f\\"   - ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_datasets)} datasets, {total_rows:,} rows\\")\\n",
        "\\n",
        "send_telegram(f\\"‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(all_datasets)} datasets, {total_rows:,} rows\\")\\n",
        "print(\\"\\\\n‚úÖ ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\n",
        "import torch\\n",
        "\\n",
        "send_telegram(\\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B\\")\\n",
        "\\n",
        "model_name = \\"EleutherAI/gpt-j-6b\\"\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î tokenizer...\\")\\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\\n",
        "tokenizer.pad_token = tokenizer.eos_token\\n",
        "\\n",
        "print(\\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\\")\\n",
        "model = AutoModelForCausalLM.from_pretrained(\\n",
        "    model_name,\\n",
        "    torch_dtype=torch.float16,\\n",
        "    device_map=\\"auto\\",\\n",
        "    token=HF_TOKEN\\n",
        ")\\n",
        "\\n",
        "# LoRA Configuration (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\\n",
        "lora_config = LoraConfig(\\n",
        "    r=32,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 16\\n",
        "    lora_alpha=64,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 32\\n",
        "    target_modules=[\\"q_proj\\", \\"v_proj\\"],\\n",
        "    lora_dropout=0.05,\\n",
        "    bias=\\"none\\",\\n",
        "    task_type=\\"CAUSAL_LM\\"\\n",
        ")\\n",
        "\\n",
        "model = get_peft_model(model, lora_config)\\n",
        "model.print_trainable_parameters()\\n",
        "\\n",
        "print(\\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\")\\n",
        "send_telegram(\\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\\")\\n",
        "\\n",
        "# Training Arguments (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\\n",
        "training_args = TrainingArguments(\\n",
        "    output_dir=\\"/content/dLNk-gpt-j-6b-agent\\",\\n",
        "    num_train_epochs=3,\\n",
        "    per_device_train_batch_size=8,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 4\\n",
        "    gradient_accumulation_steps=2,\\n",
        "    learning_rate=2e-4,\\n",
        "    fp16=True,\\n",
        "    logging_steps=10,\\n",
        "    save_steps=500,\\n",
        "    warmup_steps=100,\\n",
        "    weight_decay=0.01,\\n",
        "    report_to=\\"wandb\\" if WANDB_API_KEY else \\"none\\"\\n",
        ")\\n",
        "\\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\\n",
        "trainer = Trainer(\\n",
        "    model=model,\\n",
        "    args=training_args,\\n",
        "    train_dataset=combined_dataset,\\n",
        "    tokenizer=tokenizer\\n",
        ")\\n",
        "\\n",
        "print(\\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...\\")\\n",
        "trainer.train()\\n",
        "\\n",
        "print(\\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\\")\\n",
        "send_telegram(\\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîü ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\\"üì§ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\\")\\n",
        "\\n",
        "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\\n",
        "output_dir = \\"/content/dLNk-gpt-j-6b-agent-final\\"\\n",
        "model.save_pretrained(output_dir)\\n",
        "tokenizer.save_pretrained(output_dir)\\n",
        "\\n",
        "print(f\\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà: {output_dir}\\")\\n",
        "\\n",
        "# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub\\n",
        "try:\\n",
        "    model.push_to_hub(\\"dLNk/gpt-j-6b-agent-ultimate\\", token=HF_TOKEN)\\n",
        "    tokenizer.push_to_hub(\\"dLNk/gpt-j-6b-agent-ultimate\\", token=HF_TOKEN)\\n",
        "    print(\\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\")\\n",
        "    send_telegram(\\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: dLNk/gpt-j-6b-agent-ultimate\\")\\n",
        "except Exception as e:\\n",
        "    print(f\\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}\\")\\n",
        "    send_telegram(f\\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}\\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
