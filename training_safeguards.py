"""
‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£
- ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Overfitting
- ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Model ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á
- Auto-adjustment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameters
"""

import numpy as np
import json
from typing import Dict, List, Tuple
from collections import defaultdict
import warnings

class TrainingSafeguards:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£"""
    
    def __init__(self, patience: int = 3, min_delta: float = 0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.train_losses = []
        self.eval_losses = []
        self.learning_rates = []
        self.best_eval_loss = float('inf')
        self.patience_counter = 0
        self.warnings = []
        
    def check_overfitting(self, train_loss: float, eval_loss: float) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Overfitting
        
        Returns:
            (is_overfitting, message)
        """
        self.train_losses.append(train_loss)
        self.eval_losses.append(eval_loss)
        
        if len(self.train_losses) < 3:
            return False, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ train loss ‡∏•‡∏î‡∏•‡∏á ‡πÅ‡∏ï‡πà eval loss ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
        train_trend = np.mean(np.diff(self.train_losses[-3:]))
        eval_trend = np.mean(np.diff(self.eval_losses[-3:]))
        
        # Gap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train ‡πÅ‡∏•‡∏∞ eval loss
        gap = eval_loss - train_loss
        gap_threshold = 0.5  # ‡∏ñ‡πâ‡∏≤ gap ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 0.5 ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ overfitting
        
        is_overfitting = False
        message = ""
        
        if train_trend < 0 and eval_trend > 0:
            is_overfitting = True
            message = "‚ö†Ô∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö Overfitting: Train loss ‡∏•‡∏î‡∏•‡∏á ‡πÅ‡∏ï‡πà Eval loss ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô"
        elif gap > gap_threshold:
            is_overfitting = True
            message = f"‚ö†Ô∏è ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö Overfitting: Gap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train ‡πÅ‡∏•‡∏∞ eval = {gap:.4f} (‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ {gap_threshold})"
        else:
            message = f"‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö Overfitting (Gap = {gap:.4f})"
        
        if is_overfitting:
            self.warnings.append({
                'type': 'overfitting',
                'train_loss': train_loss,
                'eval_loss': eval_loss,
                'gap': gap,
                'message': message
            })
        
        return is_overfitting, message
    
    def check_learning_progress(self, current_loss: float) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Returns:
            (is_learning, message)
        """
        if len(self.train_losses) < 2:
            return True, "‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
        
        recent_losses = self.train_losses[-5:]  # ‡∏î‡∏π 5 epochs ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ loss ‡∏•‡∏î‡∏•‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if len(recent_losses) >= 2:
            improvement = recent_losses[0] - recent_losses[-1]
            
            if improvement < self.min_delta:
                self.patience_counter += 1
                message = f"‚ö†Ô∏è Loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á ({self.patience_counter}/{self.patience})"
                
                if self.patience_counter >= self.patience:
                    self.warnings.append({
                        'type': 'no_learning',
                        'recent_losses': recent_losses,
                        'message': "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ"
                    })
                    return False, "‚ùå ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ! ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö learning rate ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
                
                return True, message
            else:
                self.patience_counter = 0
                return True, f"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (‡∏•‡∏î‡∏•‡∏á {improvement:.4f})"
        
        return True, "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô..."
    
    def check_data_conflicts(self, dataset: List[Dict]) -> Tuple[bool, List[str]]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á‡∏Å‡∏±‡∏ô
        
        Args:
            dataset: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ 'instruction', 'input', 'output'
        
        Returns:
            (has_conflicts, conflict_messages)
        """
        conflicts = []
        instruction_outputs = defaultdict(set)
        
        # ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° outputs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ instruction+input
        for idx, item in enumerate(dataset):
            key = f"{item.get('instruction', '')}|||{item.get('input', '')}"
            output = item.get('output', '')
            instruction_outputs[key].add(output)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ instruction+input ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ output ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        for key, outputs in instruction_outputs.items():
            if len(outputs) > 1:
                instruction, inp = key.split('|||')
                conflicts.append(
                    f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á: '{instruction[:50]}...' ‡∏°‡∏µ {len(outputs)} outputs ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô"
                )
        
        if conflicts:
            self.warnings.append({
                'type': 'data_conflicts',
                'count': len(conflicts),
                'examples': conflicts[:5]  # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà 5 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏£‡∏Å
            })
        
        return len(conflicts) > 0, conflicts
    
    def suggest_learning_rate(self, current_lr: float, current_loss: float) -> Tuple[float, str]:
        """
        ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ learning rate ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
        
        Returns:
            (suggested_lr, reason)
        """
        self.learning_rates.append(current_lr)
        
        if len(self.train_losses) < 3:
            return current_lr, "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠"
        
        recent_losses = self.train_losses[-3:]
        loss_change = recent_losses[-1] - recent_losses[0]
        
        # ‡∏ñ‡πâ‡∏≤ loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡∏¢ -> ‡∏•‡∏î learning rate
        if loss_change >= 0:
            new_lr = current_lr * 0.5
            return new_lr, f"Loss ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á -> ‡∏•‡∏î LR ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ {new_lr:.2e}"
        
        # ‡∏ñ‡πâ‡∏≤ loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å -> ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏° learning rate ‡πÑ‡∏î‡πâ
        if loss_change < -0.5:
            new_lr = min(current_lr * 1.2, 5e-5)  # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5e-5
            return new_lr, f"Loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß -> ‡πÄ‡∏û‡∏¥‡πà‡∏° LR ‡πÄ‡∏õ‡πá‡∏ô {new_lr:.2e}"
        
        return current_lr, "Learning rate ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÅ‡∏•‡πâ‡∏ß"
    
    def should_stop_training(self) -> Tuple[bool, str]:
        """
        ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        
        Returns:
            (should_stop, reason)
        """
        if len(self.eval_losses) < self.patience:
            return False, "‡∏¢‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏≠"
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ eval loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á patience epochs
        recent_eval = self.eval_losses[-self.patience:]
        if min(recent_eval) >= self.best_eval_loss - self.min_delta:
            return True, f"Eval loss ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô {self.patience} epochs ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï best eval loss
        if self.eval_losses[-1] < self.best_eval_loss:
            self.best_eval_loss = self.eval_losses[-1]
        
        return False, "‡∏¢‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ"
    
    def get_training_summary(self) -> Dict:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        
        Returns:
            dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        summary = {
            'total_epochs': len(self.train_losses),
            'best_train_loss': min(self.train_losses) if self.train_losses else None,
            'best_eval_loss': self.best_eval_loss if self.best_eval_loss != float('inf') else None,
            'final_train_loss': self.train_losses[-1] if self.train_losses else None,
            'final_eval_loss': self.eval_losses[-1] if self.eval_losses else None,
            'warnings': self.warnings,
            'warning_count': len(self.warnings)
        }
        
        return summary
    
    def generate_report(self) -> str:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        """
        summary = self.get_training_summary()
        
        report = "=" * 60 + "\n"
        report += "üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n"
        report += "=" * 60 + "\n\n"
        
        report += f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epochs: {summary['total_epochs']}\n"
        report += f"Train Loss ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {summary['best_train_loss']:.4f}\n" if summary['best_train_loss'] else ""
        report += f"Eval Loss ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {summary['best_eval_loss']:.4f}\n" if summary['best_eval_loss'] else ""
        report += f"Train Loss ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {summary['final_train_loss']:.4f}\n" if summary['final_train_loss'] else ""
        report += f"Eval Loss ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {summary['final_eval_loss']:.4f}\n" if summary['final_eval_loss'] else ""
        
        report += f"\n‚ö†Ô∏è ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {summary['warning_count']} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n"
        
        if summary['warnings']:
            report += "\n‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô:\n"
            for i, warning in enumerate(summary['warnings'], 1):
                report += f"\n{i}. {warning['type'].upper()}\n"
                report += f"   {warning.get('message', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°')}\n"
        
        report += "\n" + "=" * 60 + "\n"
        
        return report


# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
def create_safeguards(patience=3, min_delta=0.001):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á TrainingSafeguards instance"""
    return TrainingSafeguards(patience=patience, min_delta=min_delta)


if __name__ == "__main__":
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    safeguards = create_safeguards()
    
    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
    print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤\n")
    
    # Epoch 1
    is_overfitting, msg = safeguards.check_overfitting(train_loss=2.5, eval_loss=2.6)
    print(f"Epoch 1: {msg}")
    
    # Epoch 2
    is_overfitting, msg = safeguards.check_overfitting(train_loss=2.0, eval_loss=2.1)
    print(f"Epoch 2: {msg}")
    
    # Epoch 3 - Overfitting
    is_overfitting, msg = safeguards.check_overfitting(train_loss=1.5, eval_loss=2.5)
    print(f"Epoch 3: {msg}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    is_learning, msg = safeguards.check_learning_progress(1.5)
    print(f"\n‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ: {msg}")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    print("\n" + safeguards.generate_report())
