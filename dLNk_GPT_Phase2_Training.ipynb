{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# üöÄ dLNk GPT Agent v2 - Phase 2: Long-form Code Generation\\n",
        "\\n",
        "**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** Fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏• dLNk-gpt-j-6b-agent-v1 ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏¢‡∏≤‡∏ß‡πÜ ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\\n",
        "\\n",
        "**Training Data:**\\n",
        "- CodeAlpaca-20k (~20,000 examples)\\n",
        "- Python Code Instructions 18k (~18,000 examples)\\n",
        "- Code Instructions 120k subset (~20,000 examples)\\n",
        "- **‡∏£‡∏ß‡∏° ~58,000 examples**\\n",
        "\\n",
        "**‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤:** 10-15 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\\n",
        "\\n",
        "**GPU:** A100 High-RAM\\n",
        "\\n",
        "---\\n",
        "\\n",
        "## üìù ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\\n",
        "\\n",
        "1. **‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GPU:** Runtime > Change runtime type > A100 GPU\\n",
        "2. **‡∏£‡∏±‡∏ô Cell 1:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\\n",
        "3. **Restart Runtime:** Runtime > Restart runtime (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)\\n",
        "4. **‡∏£‡∏±‡∏ô Cell 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ:** ‡∏Ç‡πâ‡∏≤‡∏° Cell 1\\n",
        "\\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell1"
      },
      "outputs": [],
      "source": "1: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ)\n# ================================================================\n\n!pip install -q -U pip setuptools wheel\n\n# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô ‡πÉ‡∏´‡πâ pip ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ dependencies ‡πÄ‡∏≠‡∏á\n!pip install -q -U transformers accelerate peft bitsandbytes datasets trl huggingface_hub wandb requests\n\nprint(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(\"‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Restart Runtime: Runtime > Restart runtime\")\nprint(\"‚ö†Ô∏è ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å restart ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô Cell 2 ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏õ (‡∏Ç‡πâ‡∏≤‡∏° Cell 1)\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell2"
      },
      "outputs": [],
      "source": "2: Configuration ‡πÅ‡∏•‡∏∞ Tokens\n# ================================================================\n\nimport os\nfrom datetime import datetime\n\n# ==================== ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ====================\n\n# Hugging Face Token (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå write)\nHF_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å HF Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà  # ‡∏Å‡∏£‡∏≠‡∏Å HF Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Telegram Bot Configuration\nTELEGRAM_BOT_TOKEN = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Bot Token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Bot Token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\nTELEGRAM_CHAT_ID = \"\"  # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Chat ID ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà    # ‡∏Å‡∏£‡∏≠‡∏Å Telegram Chat ID ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n\n# Weights & Biases (Optional - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á)\nWANDB_API_KEY = \"\"       # ‡∏Å‡∏£‡∏≠‡∏Å W&B API Key ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (optional)\nWANDB_PROJECT = \"dLNk-gpt-v2\"\nWANDB_RUN_NAME = f\"phase2-longform-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n\n# Model Configuration\nBASE_MODEL = \"dlnkgpt/dLNk-gpt-j-6b-agent-v1\"  # ‡πÇ‡∏°‡πÄ‡∏î‡∏• v1 ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß\nNEW_MODEL_NAME = \"dlnkgpt/dLNk-gpt-j-6b-agent-v2-phase2\"\n\n# Training Configuration\nMAX_SEQ_LENGTH = 2048  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 1024 ‡πÄ‡∏õ‡πá‡∏ô 2048 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡∏¢‡∏≤‡∏ß\nNUM_TRAIN_EPOCHS = 3\nBATCH_SIZE = 1\nGRADIENT_ACCUMULATION_STEPS = 16\nLEARNING_RATE = 2e-5\nWARMUP_STEPS = 100\nLOGGING_STEPS = 50\nSAVE_STEPS = 500\n\n# ================================================================\n\n# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ environment variables\nos.environ[\"HF_TOKEN\"] = HF_TOKEN\nif WANDB_API_KEY:\n    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n    os.environ[\"WANDB_PROJECT\"] = WANDB_PROJECT\n\nprint(\"‚úÖ Configuration ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üì¶ Base Model: {BASE_MODEL}\")\nprint(f\"üéØ New Model: {NEW_MODEL_NAME}\")\nprint(f\"üìè Max Sequence Length: {MAX_SEQ_LENGTH}\")\nprint(f\"üîÑ Epochs: {NUM_TRAIN_EPOCHS}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell3"
      },
      "outputs": [],
      "source": "3: Telegram Notification Function\n# ================================================================\n\nimport requests\nfrom datetime import datetime\n\ndef send_telegram_message(message):\n    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Telegram\"\"\"\n    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:\n        print(\"‚ö†Ô∏è Telegram not configured\")\n        return\n    \n    try:\n        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n        data = {\n            \"chat_id\": TELEGRAM_CHAT_ID,\n            \"text\": message,\n            \"parse_mode\": \"Markdown\"\n        }\n        response = requests.post(url, data=data)\n        if response.status_code == 200:\n            print(\"‚úÖ Telegram notification sent\")\n        else:\n            print(f\"‚ö†Ô∏è Telegram error: {response.status_code}\")\n    except Exception as e:\n        print(f\"‚ùå Telegram error: {e}\")\n\n# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram\nsend_telegram_message(\n    f\"üöÄ *dLNk GPT Agent v2 - Phase 2*\\\\n\\\\n\"\n    f\"üìÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\"\n    f\"üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: Long-form Code Generation\\\\n\"\n    f\"‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 10-15 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\"\n)\n\nprint(\"‚úÖ Telegram notification function ready\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell4"
      },
      "outputs": [],
      "source": "4: ‡πÇ‡∏´‡∏•‡∏î Base Model ‡πÅ‡∏•‡∏∞ Tokenizer\n# ================================================================\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, prepare_model_for_kbit_training\n\nprint(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î base model...\")\n\n# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# ‡πÇ‡∏´‡∏•‡∏î tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"EleutherAI/gpt-j-6b\",\n    trust_remote_code=True\n)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ‡πÇ‡∏´‡∏•‡∏î base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/gpt-j-6b\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# ‡πÇ‡∏´‡∏•‡∏î LoRA adapter ‡∏à‡∏≤‡∏Å v1\nprint(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î LoRA adapter ‡∏à‡∏≤‡∏Å v1...\")\nmodel = PeftModel.from_pretrained(\n    base_model,\n    BASE_MODEL,\n    token=HF_TOKEN\n)\n\n# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏ï‡πà‡∏≠\nmodel = prepare_model_for_kbit_training(model)\n\nprint(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• v1 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üîß Device: {model.device}\")\nprint(f\"üíæ Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")\n\nsend_telegram_message(\n    f\"‚úÖ *‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• v1 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à*\\\\n\\\\n\"\n    f\"üì¶ Base: EleutherAI/gpt-j-6b\\\\n\"\n    f\"üîß LoRA: {BASE_MODEL}\\\\n\"\n    f\"üíæ Memory: {model.get_memory_footprint() / 1e9:.2f} GB\"\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell5"
      },
      "outputs": [],
      "source": "5: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Training Data (‡πÉ‡∏ä‡πâ datasets ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á)\n# ================================================================\n\nfrom datasets import load_dataset\nimport random\n\nprint(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î training datasets...\")\n\n# ==================== Dataset 1: CodeAlpaca-20k (‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ) ====================\nprint(\"üì• Loading CodeAlpaca-20k...\")\n\ntry:\n    code_alpaca = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n    \n    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô format ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n    code_alpaca_data = []\n    for example in code_alpaca:\n        code_alpaca_data.append({\n            \"instruction\": example.get(\"instruction\", \"\"),\n            \"input\": example.get(\"input\", \"\"),\n            \"output\": example.get(\"output\", \"\")\n        })\n    \n    print(f\"‚úÖ Loaded {len(code_alpaca_data)} examples from CodeAlpaca-20k\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Error loading CodeAlpaca-20k: {e}\")\n    code_alpaca_data = []\n\n# ==================== Dataset 2: Python Code Instructions 18k ====================\nprint(\"üì• Loading Python Code Instructions 18k...\")\n\ntry:\n    python_code = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=\"train\")\n    \n    python_code_data = []\n    for example in python_code:\n        python_code_data.append({\n            \"instruction\": example.get(\"instruction\", \"\"),\n            \"input\": example.get(\"input\", \"\"),\n            \"output\": example.get(\"output\", \"\")\n        })\n    \n    print(f\"‚úÖ Loaded {len(python_code_data)} examples from Python Code Instructions\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Error loading Python Code Instructions: {e}\")\n    python_code_data = []\n\n# ==================== Dataset 3: Code Instructions 120k (‡πÄ‡∏≠‡∏≤‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô) ====================\nprint(\"üì• Loading Code Instructions 120k (subset)...\")\n\ntry:\n    code_instructions = load_dataset(\"iamtarun/code_instructions_120k_alpaca\", split=\"train[:20000]\")  # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 20k\n    \n    code_instructions_data = []\n    for example in code_instructions:\n        code_instructions_data.append({\n            \"instruction\": example.get(\"instruction\", \"\"),\n            \"input\": example.get(\"input\", \"\"),\n            \"output\": example.get(\"output\", \"\")\n        })\n    \n    print(f\"‚úÖ Loaded {len(code_instructions_data)} examples from Code Instructions\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Error loading Code Instructions: {e}\")\n    code_instructions_data = []\n\n# ==================== ‡∏£‡∏ß‡∏° Datasets ====================\nall_data = code_alpaca_data + python_code_data + code_instructions_data\n\n# Shuffle\nrandom.shuffle(all_data)\n\nprint(f\"\\\\nüìä Total training examples: {len(all_data)}\")\n\n# ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ output ‡∏¢‡∏≤‡∏ß‡∏û‡∏≠ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö long-form code generation)\nfiltered_data = [d for d in all_data if len(d[\"output\"]) >= 100]  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£\n\nprint(f\"üìä Filtered examples (long code): {len(filtered_data)}\")\n\n# ‡πÅ‡∏ö‡πà‡∏á train/validation\ntrain_size = int(0.9 * len(filtered_data))\ntrain_data = filtered_data[:train_size]\nval_data = filtered_data[train_size:]\n\nprint(f\"üìä Train: {len(train_data)} examples\")\nprint(f\"üìä Validation: {len(val_data)} examples\")\n\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset object\nfrom datasets import Dataset\n\ntrain_dataset = Dataset.from_list(train_data)\nval_dataset = Dataset.from_list(val_data)\n\nprint(\"‚úÖ Training data ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")\n\nsend_telegram_message(\n    f\"‚úÖ *‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô*\\\\n\\\\n\"\n    f\"üìä Training examples: {len(train_data):,}\\\\n\"\n    f\"üìä Validation examples: {len(val_data):,}\\\\n\"\n    f\"üìè Max sequence length: {MAX_SEQ_LENGTH}\"\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell6"
      },
      "outputs": [],
      "source": "6: Format Data ‡πÅ‡∏•‡∏∞ Tokenize\n# ================================================================\n\ndef format_instruction(example):\n    \"\"\"Format data ‡πÄ‡∏õ‡πá‡∏ô instruction format\"\"\"\n    instruction = example[\"instruction\"]\n    input_text = example.get(\"input\", \"\")\n    output = example[\"output\"]\n    \n    if input_text:\n        prompt = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Input:\\\\n{input_text}\\\\n\\\\n### Response:\\\\n\"\n    else:\n        prompt = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n\"\n    \n    return prompt + output + tokenizer.eos_token\n\ndef tokenize_function(examples):\n    \"\"\"Tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\"\"\"\n    text = format_instruction(examples)\n    tokenized = tokenizer(\n        text,\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH,\n        padding=\"max_length\",\n        return_tensors=None  # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ tensors ‡πÉ‡∏ô map function\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\nprint(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    remove_columns=train_dataset.column_names,\n    desc=\"Tokenizing train dataset\"\n)\n\nval_dataset = val_dataset.map(\n    tokenize_function,\n    remove_columns=val_dataset.column_names,\n    desc=\"Tokenizing validation dataset\"\n)\n\nprint(\"‚úÖ Tokenization ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell7"
      },
      "outputs": [],
      "source": "7: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Training Arguments\n# ================================================================\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./dLNk-gpt-v2-phase2\",\n    num_train_epochs=NUM_TRAIN_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    logging_steps=LOGGING_STEPS,\n    save_steps=SAVE_STEPS,\n    evaluation_strategy=\"steps\",\n    eval_steps=SAVE_STEPS,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\" if WANDB_API_KEY else \"none\",\n    run_name=WANDB_RUN_NAME if WANDB_API_KEY else None,\n    logging_dir=\"./logs\",\n    remove_unused_columns=False,\n)\n\nprint(\"‚úÖ Training arguments ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\nprint(f\"üîÑ Epochs: {NUM_TRAIN_EPOCHS}\")\nprint(f\"üì¶ Batch size: {BATCH_SIZE}\")\nprint(f\"üìà Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"üìä Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"üéØ Learning rate: {LEARNING_RATE}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell8"
      },
      "outputs": [],
      "source": "8: Custom Trainer with Telegram Notifications\n# ================================================================\n\nfrom transformers import Trainer, TrainerCallback\n\nclass TelegramCallback(TrainerCallback):\n    \"\"\"Callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á Telegram notifications\"\"\"\n    \n    def __init__(self):\n        self.start_time = datetime.now()\n        self.last_loss = None\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà log\"\"\"\n        if logs and \"loss\" in logs:\n            self.last_loss = logs[\"loss\"]\n            \n            # ‡∏™‡πà‡∏á notification ‡∏ó‡∏∏‡∏Å 50 steps\n            if state.global_step % 50 == 0:\n                elapsed = datetime.now() - self.start_time\n                total_steps = state.max_steps\n                progress = (state.global_step / total_steps) * 100\n                \n                send_telegram_message(\n                    f\"üìä *Step {state.global_step}/{total_steps}*\\\\n\\\\n\"\n                    f\"üìâ Loss: {logs['loss']:.4f}\\\\n\"\n                    f\"üìà Progress: {progress:.1f}%\\\\n\"\n                    f\"‚è±Ô∏è Elapsed: {str(elapsed).split('.')[0]}\"\n                )\n    \n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà evaluate\"\"\"\n        if metrics:\n            send_telegram_message(\n                f\"üìä *Evaluation at Step {state.global_step}*\\\\n\\\\n\"\n                f\"üìâ Eval Loss: {metrics.get('eval_loss', 'N/A'):.4f}\\\\n\"\n                f\"üéØ Train Loss: {self.last_loss:.4f}\" if self.last_loss else \"\"\n            )\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        \"\"\"‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à\"\"\"\n        elapsed = datetime.now() - self.start_time\n        send_telegram_message(\n            f\"‚úÖ *‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!*\\\\n\\\\n\"\n            f\"‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {str(elapsed).split('.')[0]}\\\\n\"\n            f\"üìä Total steps: {state.global_step}\\\\n\"\n            f\"üìâ Final loss: {self.last_loss:.4f}\" if self.last_loss else \"\"\n        )\n\n# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    callbacks=[TelegramCallback()]\n)\n\nprint(\"‚úÖ Trainer ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell9"
      },
      "outputs": [],
      "source": "9: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n# ================================================================\n\nprint(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2...\")\nprint(f\"‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\nsend_telegram_message(\n    f\"üöÄ *‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô Phase 2*\\\\n\\\\n\"\n    f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\"\n    f\"üìä Training examples: {len(train_dataset):,}\\\\n\"\n    f\"üîÑ Epochs: {NUM_TRAIN_EPOCHS}\\\\n\"\n    f\"‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì: 10-15 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\"\n)\n\n# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô\ntrainer.train()\n\nprint(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell10"
      },
      "outputs": [],
      "source": "10: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n# ================================================================\n\nprint(\"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\n# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\nmodel.save_pretrained(\"./dLNk-gpt-v2-phase2-final\")\ntokenizer.save_pretrained(\"./dLNk-gpt-v2-phase2-final\")\n\nprint(\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå\")\n\nsend_telegram_message(\n    f\"üíæ *‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•*\\\\n\\\\n\"\n    f\"üì¶ Repository: {NEW_MODEL_NAME}\\\\n\"\n    f\"‚è≥ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà...\"\n)\n\n# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face\nprint(\"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face...\")\n\ntry:\n    model.push_to_hub(\n        NEW_MODEL_NAME,\n        token=HF_TOKEN,\n        commit_message=f\"Phase 2: Long-form Code Generation - {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n    )\n    tokenizer.push_to_hub(\n        NEW_MODEL_NAME,\n        token=HF_TOKEN\n    )\n    \n    print(f\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: https://huggingface.co/{NEW_MODEL_NAME}\")\n    \n    send_telegram_message(\n        f\"‚úÖ *‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!*\\\\n\\\\n\"\n        f\"üîó https://huggingface.co/{NEW_MODEL_NAME}\\\\n\\\\n\"\n        f\"üéâ Phase 2 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\"\n    )\n    \nexcept Exception as e:\n    print(f\"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î: {e}\")\n    \n    send_telegram_message(\n        f\"‚ö†Ô∏è *‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î*\\\\n\\\\n\"\n        f\"‚ùå Error: {str(e)}\\\\n\\\\n\"\n        f\"üíæ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: ./dLNk-gpt-v2-phase2-final\"\n    )\n\nprint(\"üéâ Phase 2 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell11"
      },
      "outputs": [],
      "source": "11: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Optional)\n# ================================================================\n\nprint(\"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n\ntest_prompt = \"### Instruction:\\\\nWrite a comprehensive Python script for SQL injection testing:\\\\n\\\\n### Response:\\\\n\"\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=512,\n    temperature=0.7,\n    do_sample=True,\n    top_p=0.9\n)\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"üß™ Test Generation:\")\nprint(\"=\"*80)\nprint(response)\nprint(\"=\"*80)\n\nsend_telegram_message(\n    f\"üß™ *‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•*\\\\n\\\\n\"\n    f\"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ generate ‡πÇ‡∏Ñ‡πâ‡∏î‡πÑ‡∏î‡πâ\\\\n\"\n    f\"üìù ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\"\n)\n\nprint(\"\\\\n‚úÖ ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\")"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}