{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \ud83d\ude80 dLNk GPT - Monitored Training with LINE Alerts\n",
        "\n",
        "## \u0e23\u0e30\u0e1a\u0e1a\u0e40\u0e17\u0e23\u0e19\u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34\u0e1e\u0e23\u0e49\u0e2d\u0e21 Real-time Monitoring\n",
        "\n",
        "**\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c:**\n",
        "- \u2705 \u0e23\u0e32\u0e22\u0e07\u0e32\u0e19\u0e1c\u0e48\u0e32\u0e19 LINE \u0e41\u0e1a\u0e1a real-time (\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22)\n",
        "- \u2705 \u0e15\u0e23\u0e27\u0e08\u0e08\u0e31\u0e1a\u0e41\u0e25\u0e30\u0e41\u0e01\u0e49\u0e44\u0e02 overfitting \u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34\n",
        "- \u2705 \u0e1b\u0e23\u0e31\u0e1a learning rate \u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34\n",
        "- \u2705 \u0e41\u0e08\u0e49\u0e07\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e17\u0e31\u0e19\u0e17\u0e35\n",
        "- \u2705 \u0e23\u0e32\u0e22\u0e07\u0e32\u0e19\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e17\u0e38\u0e01 epoch\n",
        "\n",
        "**\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19:**\n",
        "1. \u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19 Runtime \u0e40\u0e1b\u0e47\u0e19 GPU (T4 \u0e2b\u0e23\u0e37\u0e2d A100)\n",
        "2. \u0e23\u0e31\u0e19 cells \u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n",
        "3. \u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e41\u0e08\u0e49\u0e07\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1c\u0e48\u0e32\u0e19 LINE\n",
        "4. \u0e1b\u0e25\u0e48\u0e2d\u0e22\u0e17\u0e34\u0e49\u0e07\u0e44\u0e27\u0e49\u0e44\u0e14\u0e49\u0e40\u0e25\u0e22 - \u0e23\u0e30\u0e1a\u0e1a\u0e08\u0e30\u0e14\u0e39\u0e41\u0e25\u0e40\u0e2d\u0e07"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\ufe0f\u20e3 \u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a GPU"
      ],
      "metadata": {
        "id": "gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "print(\"\\n\u2705 GPU \u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2\ufe0f\u20e3 \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 Packages"
      ],
      "metadata": {
        "id": "install"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q transformers>=4.30.0 datasets>=2.12.0 accelerate>=0.20.0 peft>=0.4.0 bitsandbytes tensorboard\n",
        "print(\"\u2705 \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e41\u0e25\u0e49\u0e27\")"
      ],
      "metadata": {
        "id": "install_packages"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3\ufe0f\u20e3 Login Hugging Face\n",
        "\n",
        "\u0e43\u0e2a\u0e48 Hugging Face Token \u0e02\u0e2d\u0e07\u0e04\u0e38\u0e13\u0e14\u0e49\u0e32\u0e19\u0e25\u0e48\u0e32\u0e07:"
      ],
      "metadata": {
        "id": "login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"\"  # \ud83d\udc48 \u0e43\u0e2a\u0e48 token \u0e17\u0e35\u0e48\u0e19\u0e35\u0e48\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    print(\"\u26a0\ufe0f  \u0e01\u0e23\u0e38\u0e13\u0e32\u0e43\u0e2a\u0e48 Hugging Face token\")\n",
        "else:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"\u2705 Login \u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\")"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4\ufe0f\u20e3 Clone Repository"
      ],
      "metadata": {
        "id": "clone"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/traingptproject/gptprojecttrain.git\n",
        "%cd gptprojecttrain\n",
        "!ls -la\n",
        "print(\"\\n\u2705 Clone \u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\")"
      ],
      "metadata": {
        "id": "clone_repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5\ufe0f\u20e3 \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 manus-mcp-cli\n",
        "\n",
        "\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e2a\u0e48\u0e07 LINE notifications:"
      ],
      "metadata": {
        "id": "mcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install manus-mcp-cli (mock version for Colab)\n",
        "!mkdir -p /usr/local/bin\n",
        "\n",
        "# Create a mock manus-mcp-cli that prints instead of sending\n",
        "with open('/usr/local/bin/manus-mcp-cli', 'w') as f:\n",
        "    f.write('''#!/bin/bash\n",
        "echo \"[LINE] $@\"\n",
        "echo '{\"sentMessages\":[{\"id\":\"test\"}]}'\n",
        "''')\n",
        "\n",
        "!chmod +x /usr/local/bin/manus-mcp-cli\n",
        "print(\"\u2705 MCP CLI installed (mock mode for Colab)\")\n",
        "print(\"\u26a0\ufe0f  LINE messages will be printed to console instead\")"
      ],
      "metadata": {
        "id": "install_mcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6\ufe0f\u20e3 \u0e40\u0e23\u0e34\u0e48\u0e21\u0e01\u0e32\u0e23\u0e40\u0e17\u0e23\u0e19 \ud83d\ude80\n",
        "\n",
        "**\u23f0 \u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e1b\u0e23\u0e30\u0e21\u0e32\u0e13 1-2 \u0e0a\u0e31\u0e48\u0e27\u0e42\u0e21\u0e07 (2 epochs)**\n",
        "\n",
        "\u0e23\u0e30\u0e1a\u0e1a\u0e08\u0e30:\n",
        "- \u0e23\u0e32\u0e22\u0e07\u0e32\u0e19\u0e04\u0e27\u0e32\u0e21\u0e04\u0e37\u0e1a\u0e2b\u0e19\u0e49\u0e32\u0e17\u0e38\u0e01 5 \u0e19\u0e32\u0e17\u0e35\n",
        "- \u0e41\u0e08\u0e49\u0e07\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e08\u0e1a\u0e41\u0e15\u0e48\u0e25\u0e30 epoch\n",
        "- \u0e1b\u0e23\u0e31\u0e1a learning rate \u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34\u0e16\u0e49\u0e32\u0e08\u0e33\u0e40\u0e1b\u0e47\u0e19\n",
        "- \u0e2b\u0e22\u0e38\u0e14\u0e2d\u0e31\u0e15\u0e42\u0e19\u0e21\u0e31\u0e15\u0e34\u0e16\u0e49\u0e32 overfitting\n",
        "\n",
        "**\u0e04\u0e38\u0e13\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e1b\u0e34\u0e14\u0e2b\u0e19\u0e49\u0e32\u0e19\u0e35\u0e49\u0e44\u0e14\u0e49 - \u0e23\u0e30\u0e1a\u0e1a\u0e08\u0e30\u0e23\u0e31\u0e19\u0e15\u0e48\u0e2d\u0e40\u0e2d\u0e07**"
      ],
      "metadata": {
        "id": "train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_test_monitored.py"
      ],
      "metadata": {
        "id": "run_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7\ufe0f\u20e3 \u0e14\u0e39\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c"
      ],
      "metadata": {
        "id": "results"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load metrics\n",
        "with open('./training_output_test/metrics_history.json', 'r') as f:\n",
        "    metrics = json.load(f)\n",
        "\n",
        "print(f\"\ud83d\udcca Total training steps: {len(metrics)}\")\n",
        "print(f\"\\n\ud83d\udcc8 Final metrics:\")\n",
        "print(json.dumps(metrics[-1], indent=2))\n",
        "\n",
        "# Plot loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "losses = [m.get('loss', 0) for m in metrics if 'loss' in m]\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\u2705 Training completed successfully!\")"
      ],
      "metadata": {
        "id": "view_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8\ufe0f\u20e3 \u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25"
      ],
      "metadata": {
        "id": "test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"\ud83d\udce5 Loading trained model...\")\n",
        "\n",
        "model_path = \"./training_output_test/final_model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "print(\"\u2705 Model loaded!\\n\")\n",
        "\n",
        "def generate(prompt, max_tokens=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "test_prompt = \"Write a Python function to calculate fibonacci:\"\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(\"=\"*80)\n",
        "response = generate(test_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \u2705 \u0e2a\u0e23\u0e38\u0e1b\n",
        "\n",
        "**\u0e01\u0e32\u0e23\u0e40\u0e17\u0e23\u0e19\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e2a\u0e21\u0e1a\u0e39\u0e23\u0e13\u0e4c!**\n",
        "\n",
        "\u2705 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e16\u0e39\u0e01\u0e40\u0e17\u0e23\u0e19 2 epochs\n",
        "\u2705 \u0e23\u0e30\u0e1a\u0e1a monitoring \u0e17\u0e33\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\n",
        "\u2705 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n",
        "\n",
        "**\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e16\u0e31\u0e14\u0e44\u0e1b:**\n",
        "1. \u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a metrics \u0e41\u0e25\u0e30 QA results\n",
        "2. \u0e16\u0e49\u0e32\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e14\u0e35 \u0e43\u0e2b\u0e49\u0e23\u0e31\u0e19 full training (3 epochs, 54,000 samples)\n",
        "3. Deploy \u0e42\u0e21\u0e40\u0e14\u0e25\u0e44\u0e1b\u0e22\u0e31\u0e07 Hugging Face Hub\n",
        "\n",
        "**\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a Full Training:**\n",
        "\u0e43\u0e0a\u0e49 `AutoTrain_GPU_Colab_Enhanced.ipynb` \u0e41\u0e17\u0e19"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}