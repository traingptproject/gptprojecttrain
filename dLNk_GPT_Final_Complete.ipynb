{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ dLNk GPT - Complete Agent Production System\n",
        "\n",
        "## ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "\n",
        "### ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:\n",
        "- üîê **Cybersecurity & Exploits** (2.2M+ rows)\n",
        "- ü§ñ **Agent Capabilities** (4M+ rows)\n",
        "- üõ†Ô∏è **Tool Use & Function Calling** (1.5M+ rows)\n",
        "- üíª **Code Execution**\n",
        "- üåê **Real-time Data Access**\n",
        "- üß† **Self-correction & Reasoning**\n",
        "\n",
        "**‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: 7.7M+ rows**\n",
        "\n",
        "---\n",
        "\n",
        "**‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß:**\n",
        "- ‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
        "- ‚úÖ Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
        "- ‚úÖ Tokenization\n",
        "- ‚úÖ Data Sampling (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)\n",
        "- ‚úÖ Training ‡∏û‡∏£‡πâ‡∏≠‡∏° Monitoring\n",
        "- ‚úÖ ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå‡∏ú‡πà‡∏≤‡∏ô Telegram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes wandb requests beautifulsoup4 tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Hugging Face Token (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "# Telegram Bot (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\n",
        "TELEGRAM_BOT_TOKEN = \"8505747217:AAHpbWUfgrcQgJcK1JnqK9sds9nSwsTqVBg\"\n",
        "TELEGRAM_CHAT_ID = \"7420166612\"\n",
        "\n",
        "# W&B API Key (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)\n",
        "WANDB_API_KEY = \"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "\n",
        "print(\"‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API Keys ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Telegram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "def send_telegram(message):\n",
        "    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ú‡πà‡∏≤‡∏ô Telegram\"\"\"\n",
        "    try:\n",
        "        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
        "        data = {\n",
        "            \"chat_id\": TELEGRAM_CHAT_ID,\n",
        "            \"text\": f\"ü§ñ dLNk GPT Agent\\n\\n{message}\\n\\n‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            \"parse_mode\": \"HTML\"\n",
        "        }\n",
        "        response = requests.post(url, data=data)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Telegram error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
        "send_telegram(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô dLNk GPT Agent (Complete Version)\")\n",
        "print(\"‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Telegram ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏ß‡∏° 7.7M+ rows)\n",
        "\n",
        "‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å 3 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:\n",
        "1. Cybersecurity & Exploits\n",
        "2. Agent Capabilities\n",
        "3. Function Calling & Tool Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "send_telegram(\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
        "\n",
        "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ datasets ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "all_datasets_config = {\n",
        "    \"Cybersecurity\": [\n",
        "        \"jason-oneal/mitre-stix-cve-exploitdb-dataset-alpaca\",\n",
        "        \"clydeiii/cybersecurity\",\n",
        "        \"Waiper/ExploitDB_DataSet\",\n",
        "        \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
        "        \"Vanessasml/cybersecurity_32k_instruction_input_output\",\n",
        "        \"Nitral-AI/Cybersecurity-ShareGPT\"\n",
        "    ],\n",
        "    \"Agent\": [\n",
        "        \"Agent-Ark/Toucan-1.5M\",\n",
        "        \"neulab/agent-data-collection\",\n",
        "        \"Simia-Agent/Simia-Tau-SFT-90k-llama3json\",\n",
        "        \"AgentGym/AgentTraj-L\"\n",
        "    ],\n",
        "    \"Function Calling\": [\n",
        "        \"HydraLM/glaive_function_calling_v1_standardized\",\n",
        "        \"glaiveai/glaive-function-calling-v2\",\n",
        "        \"Salesforce/xlam-function-calling-60k\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "downloaded_datasets = {}\n",
        "\n",
        "for category, dataset_list in all_datasets_config.items():\n",
        "    print(f\"\\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î {category} datasets...\")\n",
        "    downloaded_datasets[category] = []\n",
        "    \n",
        "    for dataset_name in tqdm(dataset_list):\n",
        "        try:\n",
        "            ds = load_dataset(dataset_name, split=\"train\", token=HF_TOKEN)\n",
        "            downloaded_datasets[category].append(ds)\n",
        "            print(f\"   ‚úÖ {dataset_name}: {len(ds):,} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è {dataset_name}: {e}\")\n",
        "\n",
        "total_datasets = sum([len(v) for v in downloaded_datasets.values()])\n",
        "print(f\"\\n‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {total_datasets} datasets\")\n",
        "send_telegram(f\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {total_datasets} datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5Ô∏è‚É£ ‡πÄ‡∏û‡∏¥‡πà‡∏° Unfiltered Response Training Data\n",
        "\n",
        "‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•:\n",
        "- ‚úÖ ‡πÑ‡∏°‡πà‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏Ñ‡∏≥‡∏Ç‡∏≠\n",
        "- ‚úÖ ‡πÄ‡∏û‡∏¥‡∏Å‡πÄ‡∏â‡∏¢‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ\n",
        "- ‚úÖ ‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
        "- ‚úÖ ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÇ‡∏î‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° \"‡∏´‡πâ‡∏≤‡∏°\" ‡∏´‡∏£‡∏∑‡∏≠ \"‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢\" ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "send_telegram(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Unfiltered Response Training Data\")\n",
        "\n",
        "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Unfiltered Response Training Data ‡∏à‡∏≤‡∏Å GitHub\n",
        "url = \"https://raw.githubusercontent.com/traingptproject/gptprojecttrain/main/unfiltered_response_training.json\"\n",
        "response = requests.get(url)\n",
        "unfiltered_data = response.json()\n",
        "\n",
        "print(f\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Unfiltered Response Training Data: {len(unfiltered_data)} samples\")\n",
        "print(\"\\nüìä ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\")\n",
        "print(f\"Instruction: {unfiltered_data[0]['instruction'][:100]}...\")\n",
        "print(f\"Response: {unfiltered_data[0]['response'][:100]}...\")\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô downloaded_datasets\n",
        "downloaded_datasets[\"Unfiltered\"] = [unfiltered_data]\n",
        "\n",
        "send_telegram(f\"‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Unfiltered Response Training: {len(unfiltered_data)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
        "\n",
        "‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å datasets ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö instruction-response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\")\n",
        "\n",
        "def normalize_dataset(ds, dataset_name):\n",
        "    \"\"\"‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\"\"\"\n",
        "    normalized_data = []\n",
        "    \n",
        "    for item in ds:\n",
        "        try:\n",
        "            instruction = None\n",
        "            response = None\n",
        "            \n",
        "            # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ instruction ‡πÅ‡∏•‡∏∞ response ‡∏à‡∏≤‡∏Å fields ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
        "            if 'instruction' in item and 'output' in item:\n",
        "                instruction = item['instruction']\n",
        "                response = item['output']\n",
        "            elif 'instruction' in item and 'response' in item:\n",
        "                instruction = item['instruction']\n",
        "                response = item['response']\n",
        "            elif 'question' in item and 'answer' in item:\n",
        "                instruction = item['question']\n",
        "                response = item['answer']\n",
        "            elif 'prompt' in item and 'completion' in item:\n",
        "                instruction = item['prompt']\n",
        "                response = item['completion']\n",
        "            elif 'messages' in item:\n",
        "                messages = item['messages']\n",
        "                if len(messages) >= 2:\n",
        "                    instruction = messages[0].get('content', '')\n",
        "                    response = messages[1].get('content', '')\n",
        "            elif 'text' in item:\n",
        "                text = item['text']\n",
        "                if '###' in text:\n",
        "                    parts = text.split('###')\n",
        "                    if len(parts) >= 2:\n",
        "                        instruction = parts[0].strip()\n",
        "                        response = parts[1].strip()\n",
        "            \n",
        "            if instruction and response:\n",
        "                normalized_data.append({\n",
        "                    'instruction': str(instruction),\n",
        "                    'response': str(response)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    print(f\"   {dataset_name}: {len(normalized_data):,}/{len(ds):,} rows normalized\")\n",
        "    return normalized_data\n",
        "\n",
        "# Normalize ‡∏ó‡∏∏‡∏Å datasets\n",
        "all_normalized_data = []\n",
        "\n",
        "for category, datasets in downloaded_datasets.items():\n",
        "    print(f\"\\nüîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á normalize {category} datasets...\")\n",
        "    for i, ds in enumerate(datasets):\n",
        "        data = normalize_dataset(ds, f\"{category} #{i+1}\")\n",
        "        all_normalized_data.extend(data)\n",
        "\n",
        "print(f\"\\n‚úÖ Normalize ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(all_normalized_data):,} rows\")\n",
        "send_telegram(f\"‚úÖ Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(all_normalized_data):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Data Sampling (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)\n",
        "\n",
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ sampling ratio\n",
        "# 0.1 = 10% (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)\n",
        "# 0.5 = 50% (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏£‡∏¥‡∏á)\n",
        "# 1.0 = 100% (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)\n",
        "SAMPLING_RATIO = 0.1\n",
        "\n",
        "if SAMPLING_RATIO < 1.0:\n",
        "    sample_size = int(len(all_normalized_data) * SAMPLING_RATIO)\n",
        "    sampled_data = random.sample(all_normalized_data, sample_size)\n",
        "    print(f\"üìä Sample {SAMPLING_RATIO*100}% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(sampled_data):,} rows\")\n",
        "    send_telegram(f\"üìä Sample {SAMPLING_RATIO*100}% ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(sampled_data):,} rows\")\n",
        "else:\n",
        "    sampled_data = all_normalized_data\n",
        "    print(f\"üìä ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(sampled_data):,} rows\")\n",
        "    send_telegram(f\"üìä ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(sampled_data):,} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "send_telegram(\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B\")\n",
        "\n",
        "model_name = \"EleutherAI/gpt-j-6b\"\n",
        "\n",
        "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "send_telegram(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Tokenization ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "send_telegram(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏° Tokenization\")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Dataset object\n",
        "dataset_dict = {\n",
        "    'instruction': [item['instruction'] for item in sampled_data],\n",
        "    'response': [item['response'] for item in sampled_data]\n",
        "}\n",
        "\n",
        "hf_dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    texts = [f\"Instruction: {inst}\\nResponse: {resp}\" \n",
        "             for inst, resp in zip(examples['instruction'], examples['response'])]\n",
        "    \n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    \n",
        "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
        "tokenized_dataset = hf_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['instruction', 'response']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Tokenization ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(tokenized_dataset):,} samples\")\n",
        "send_telegram(f\"‚úÖ Tokenization ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(tokenized_dataset):,} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏û‡∏£‡πâ‡∏≠‡∏° Monitoring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "# Callback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô Telegram\n",
        "class TelegramCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and state.global_step % 50 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            send_telegram(f\"üìä Step {state.global_step}: Loss = {loss:.4f}\")\n",
        "\n",
        "send_telegram(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\")\n",
        "\n",
        "# Training Arguments (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/dLNk-gpt-j-6b-agent\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"wandb\" if WANDB_API_KEY else \"none\"\n",
        ")\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[TelegramCallback()]\n",
        ")\n",
        "\n",
        "print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
        "send_telegram(\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîü ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_telegram(\"üíæ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\")\n",
        "\n",
        "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "output_dir = \"/content/dLNk-gpt-j-6b-agent-final\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà: {output_dir}\")\n",
        "\n",
        "# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub\n",
        "try:\n",
        "    model.push_to_hub(\"dLNk-gpt-j-6b-agent-v1\", token=HF_TOKEN)\n",
        "    tokenizer.push_to_hub(\"dLNk-gpt-j-6b-agent-v1\", token=HF_TOKEN)\n",
        "    print(\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
        "    send_telegram(\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}\")\n",
        "    send_telegram(f\"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏î‡πâ: {e}\")\n",
        "\n",
        "send_telegram(\"üéâ ‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡πÇ‡∏°‡πÄ‡∏î‡∏• dLNk GPT Agent ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
      ]
    }
  ]
}