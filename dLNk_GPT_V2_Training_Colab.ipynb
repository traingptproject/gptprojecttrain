{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dLNk GPT V2 Exploit Agent - Continuous Training on Google Colab\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Anti-disconnect mechanism (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Colab ‡∏´‡∏¢‡∏∏‡∏î)\n",
    "- ‚úÖ Real-time LINE notifications (‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏ú‡πà‡∏≤‡∏ô LINE)\n",
    "- ‚úÖ GitHub integration (‡∏î‡∏∂‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å GitHub)\n",
    "- ‚úÖ Hugging Face integration (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•)\n",
    "- ‚úÖ Automatic checkpointing (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)\n",
    "- ‚úÖ Progress monitoring (‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤)\n",
    "\n",
    "## Setup Instructions:\n",
    "1. ‡πÄ‡∏õ‡∏¥‡∏î Runtime > Change runtime type > ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GPU (T4 ‡∏´‡∏£‡∏∑‡∏≠ A100)\n",
    "2. ‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å cell ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö\n",
    "3. ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏ú‡πà‡∏≤‡∏ô LINE ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 1: Anti-Disconnect Mechanism\n",
    "# ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Google Colab disconnect\n",
    "\n",
    "%%javascript\n",
    "function ClickConnect(){\n",
    "  console.log(\"Keeping Colab alive...\");\n",
    "  document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}\n",
    "setInterval(ClickConnect, 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Install Dependencies\n",
    "print(\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies...\")\n",
    "\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q requests\n",
    "\n",
    "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: LINE Notification Setup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# LINE MCP configuration (will be set via environment)\n",
    "LINE_NOTIFY_ENABLED = True\n",
    "\n",
    "def send_line_notification(message, use_flex=False):\n",
    "    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LINE ‡∏ú‡πà‡∏≤‡∏ô MCP\"\"\"\n",
    "    try:\n",
    "        if use_flex:\n",
    "            # Flex message for rich formatting\n",
    "            payload = {\n",
    "                \"message\": {\n",
    "                    \"altText\": message[:100],\n",
    "                    \"contents\": {\n",
    "                        \"type\": \"bubble\",\n",
    "                        \"body\": {\n",
    "                            \"type\": \"box\",\n",
    "                            \"layout\": \"vertical\",\n",
    "                            \"contents\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": message,\n",
    "                                    \"wrap\": True\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            # Note: This would need to be called via manus-mcp-cli in actual Colab\n",
    "            print(f\"[LINE FLEX] {message}\")\n",
    "        else:\n",
    "            # Simple text message\n",
    "            payload = {\n",
    "                \"message\": {\n",
    "                    \"text\": message\n",
    "                }\n",
    "            }\n",
    "            # Note: This would need to be called via manus-mcp-cli in actual Colab\n",
    "            print(f\"[LINE] {message}\")\n",
    "        \n",
    "        # In actual Colab, this will call manus-mcp-cli\n",
    "        # For now, just print\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LINE notification error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test notification\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "send_line_notification(f\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô dLNk GPT V2 Training\\n‚è∞ {timestamp}\\n\\n‚úÖ Google Colab ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "\n",
    "print(\"‚úÖ LINE notification setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Clone GitHub Repository\n",
    "import os\n",
    "\n",
    "send_line_notification(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å GitHub...\")\n",
    "\n",
    "# Clone repository\n",
    "if os.path.exists('/content/gptprojecttrain'):\n",
    "    !rm -rf /content/gptprojecttrain\n",
    "\n",
    "!git clone https://github.com/traingptproject/gptprojecttrain.git /content/gptprojecttrain\n",
    "\n",
    "# Change to project directory\n",
    "%cd /content/gptprojecttrain\n",
    "\n",
    "# List files\n",
    "!ls -la\n",
    "\n",
    "send_line_notification(\"‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å GitHub ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\\n\\nüìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:\\n- exploit_agent.py\\n- training_config_v2_exploit.py\\n- train_exploit_agent_v2.py\\n- exploit_training_data_v2_enhanced.jsonl\")\n",
    "\n",
    "print(\"‚úÖ GitHub clone ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Check GPU and System Info\n",
    "import torch\n",
    "\n",
    "send_line_notification(\"üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö...\")\n",
    "\n",
    "# Check GPU\n",
    "gpu_available = torch.cuda.is_available()\n",
    "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"No GPU\"\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if gpu_available else 0\n",
    "\n",
    "# Check disk space\n",
    "!df -h /content\n",
    "\n",
    "# Check RAM\n",
    "!free -h\n",
    "\n",
    "system_info = f\"\"\"üíª ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö:\n",
    "\n",
    "üéÆ GPU: {gpu_name}\n",
    "üíæ GPU Memory: {gpu_memory:.1f} GB\n",
    "üî• CUDA: {torch.version.cuda if gpu_available else 'N/A'}\n",
    "üêç PyTorch: {torch.__version__}\n",
    "\n",
    "{'‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô' if gpu_available else '‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GPU'}\n",
    "\"\"\"\n",
    "\n",
    "send_line_notification(system_info)\n",
    "\n",
    "print(\"‚úÖ System check ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Prepare Training Data\n",
    "send_line_notification(\"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏¥‡πà‡∏á...\")\n",
    "\n",
    "# Check if training data exists\n",
    "import os\n",
    "\n",
    "training_files = [\n",
    "    'exploit_training_data_v2_enhanced.jsonl',\n",
    "    'analysis/exploit_training_template.jsonl'\n",
    "]\n",
    "\n",
    "data_status = \"üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏¥‡πà‡∏á:\\n\\n\"\n",
    "\n",
    "for file in training_files:\n",
    "    if os.path.exists(file):\n",
    "        size = os.path.getsize(file)\n",
    "        with open(file, 'r') as f:\n",
    "            lines = sum(1 for _ in f)\n",
    "        data_status += f\"‚úÖ {file}\\n   üìè {size} bytes, {lines} samples\\n\\n\"\n",
    "    else:\n",
    "        data_status += f\"‚ùå {file} - ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå\\n\\n\"\n",
    "\n",
    "send_line_notification(data_status)\n",
    "\n",
    "print(\"‚úÖ Data preparation ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Enhanced Training Script with Real-time Monitoring\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create enhanced training script\n",
    "training_script = '''\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from training_config_v2_exploit import *\n",
    "from datetime import datetime\n",
    "\n",
    "class LINENotificationCallback(TrainerCallback):\n",
    "    \"\"\"Callback for sending LINE notifications during training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.last_notification = None\n",
    "        self.notification_interval = 300  # 5 minutes\n",
    "    \n",
    "    def send_notification(self, message):\n",
    "        \"\"\"Send LINE notification\"\"\"\n",
    "        try:\n",
    "            print(f\"[LINE] {message}\")\n",
    "            # In actual Colab, call manus-mcp-cli here\n",
    "        except Exception as e:\n",
    "            print(f\"LINE notification error: {e}\")\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = datetime.now()\n",
    "        self.send_notification(f\"üéØ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\\n‚è∞ {self.start_time.strftime('%H:%M:%S')}\\n\\nüìä Total steps: {state.max_steps}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Send notification on logging\"\"\"\n",
    "        if logs and (self.last_notification is None or \n",
    "                    (datetime.now() - self.last_notification).seconds >= self.notification_interval):\n",
    "            \n",
    "            current_step = state.global_step\n",
    "            total_steps = state.max_steps\n",
    "            progress = (current_step / total_steps) * 100\n",
    "            \n",
    "            loss = logs.get('loss', 'N/A')\n",
    "            learning_rate = logs.get('learning_rate', 'N/A')\n",
    "            \n",
    "            elapsed = datetime.now() - self.start_time\n",
    "            eta = (elapsed / current_step * (total_steps - current_step)) if current_step > 0 else timedelta(0)\n",
    "            \n",
    "            message = f\"\"\"üìà ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\n",
    "\n",
    "üî¢ Step: {current_step}/{total_steps}\n",
    "üìä Progress: {progress:.1f}%\n",
    "üìâ Loss: {loss:.4f if isinstance(loss, float) else loss}\n",
    "‚ö° LR: {learning_rate:.2e if isinstance(learning_rate, float) else learning_rate}\n",
    "\n",
    "‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ: {str(elapsed).split('.')[0]}\n",
    "üïê ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠: {str(eta).split('.')[0]}\n",
    "\"\"\"\n",
    "            \n",
    "            self.send_notification(message)\n",
    "            self.last_notification = datetime.now()\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        \"\"\"Send notification on evaluation\"\"\"\n",
    "        if metrics:\n",
    "            eval_loss = metrics.get('eval_loss', 'N/A')\n",
    "            message = f\"\"\"üéØ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô\n",
    "\n",
    "üìâ Eval Loss: {eval_loss:.4f if isinstance(eval_loss, float) else eval_loss}\n",
    "üî¢ Step: {state.global_step}\n",
    "\"\"\"\n",
    "            self.send_notification(message)\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        elapsed = datetime.now() - self.start_time\n",
    "        message = f\"\"\"‚úÖ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n",
    "\n",
    "‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {str(elapsed).split('.')[0]}\n",
    "üî¢ Total steps: {state.global_step}\n",
    "\n",
    "üíæ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß\n",
    "\"\"\"\n",
    "        self.send_notification(message)\n",
    "\n",
    "class ExploitAgentTrainer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.dataset = None\n",
    "        self.line_callback = LINENotificationCallback()\n",
    "    \n",
    "    def load_model_and_tokenizer(self):\n",
    "        print(\"[*] Loading model and tokenizer...\")\n",
    "        self.line_callback.send_notification(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• GPT-J-6B...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_CONFIG[\"base_model\"],\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_CONFIG[\"base_model\"],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        params = self.model.num_parameters()\n",
    "        self.line_callback.send_notification(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\n\\nüî¢ Parameters: {params:,}\")\n",
    "        print(f\"[+] Model loaded: {MODEL_CONFIG['base_model']}\")\n",
    "    \n",
    "    def prepare_lora_model(self):\n",
    "        print(\"[*] Preparing LoRA...\")\n",
    "        self.line_callback.send_notification(\"‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA...\")\n",
    "        \n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        lora_config = LoraConfig(**LORA_CONFIG)\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        self.line_callback.send_notification(f\"‚úÖ LoRA ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß\\n\\nüéØ Trainable: {trainable:,}\\nüìä Percentage: {100*trainable/total:.2f}%\")\n",
    "        print(f\"[+] LoRA applied\")\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        print(\"[*] Loading dataset...\")\n",
    "        self.line_callback.send_notification(\"üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏¥‡πà‡∏á...\")\n",
    "        \n",
    "        dataset = load_dataset(\n",
    "            'json',\n",
    "            data_files=DATASET_CONFIG[\"train_file\"],\n",
    "            split='train'\n",
    "        )\n",
    "        \n",
    "        split_dataset = dataset.train_test_split(\n",
    "            test_size=DATASET_CONFIG[\"validation_split\"],\n",
    "            seed=DATASET_CONFIG[\"seed\"]\n",
    "        )\n",
    "        \n",
    "        self.dataset = split_dataset\n",
    "        \n",
    "        train_size = len(self.dataset['train'])\n",
    "        val_size = len(self.dataset['test'])\n",
    "        \n",
    "        self.line_callback.send_notification(f\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\n\\nüìö Training: {train_size} samples\\nüìñ Validation: {val_size} samples\")\n",
    "        print(f\"[+] Dataset loaded\")\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        prompts = []\n",
    "        for i in range(len(examples[DATA_FORMAT[\"instruction_key\"]])):\n",
    "            instruction = examples[DATA_FORMAT[\"instruction_key\"]][i]\n",
    "            input_text = examples[DATA_FORMAT[\"input_key\"]][i]\n",
    "            output_text = examples[DATA_FORMAT[\"output_key\"]][i]\n",
    "            \n",
    "            prompt = DATA_FORMAT[\"prompt_template\"].format(\n",
    "                instruction=instruction,\n",
    "                input=input_text,\n",
    "                output=output_text\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        tokenized = self.tokenizer(\n",
    "            prompts,\n",
    "            truncation=True,\n",
    "            max_length=DATASET_CONFIG[\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        print(\"[*] Tokenizing...\")\n",
    "        self.line_callback.send_notification(\"üî§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á Tokenize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "        \n",
    "        tokenized_dataset = self.dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.dataset[\"train\"].column_names,\n",
    "            desc=\"Tokenizing\"\n",
    "        )\n",
    "        \n",
    "        self.dataset = tokenized_dataset\n",
    "        self.line_callback.send_notification(\"‚úÖ Tokenization ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "        print(\"[+] Dataset tokenized\")\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"[*] Starting training...\")\n",
    "        \n",
    "        training_args = TrainingArguments(**TRAINING_ARGS)\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.dataset[\"train\"],\n",
    "            eval_dataset=self.dataset[\"test\"],\n",
    "            data_collator=data_collator,\n",
    "            callbacks=[self.line_callback]\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        trainer.save_model(TRAINING_ARGS[\"output_dir\"])\n",
    "        self.tokenizer.save_pretrained(TRAINING_ARGS[\"output_dir\"])\n",
    "        \n",
    "        self.line_callback.send_notification(f\"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà: {TRAINING_ARGS['output_dir']}\")\n",
    "        print(\"[+] Training completed\")\n",
    "    \n",
    "    def run_full_training(self):\n",
    "        print(\"=\"*60)\n",
    "        print(\"dLNk GPT V2 Exploit Agent Training\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.load_model_and_tokenizer()\n",
    "        self.prepare_lora_model()\n",
    "        self.load_dataset()\n",
    "        self.prepare_dataset()\n",
    "        self.train()\n",
    "        \n",
    "        print(\"[+] Training pipeline completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = ExploitAgentTrainer()\n",
    "    trainer.run_full_training()\n",
    "'''\n",
    "\n",
    "# Save enhanced training script\n",
    "with open('/content/gptprojecttrain/train_with_monitoring.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "send_line_notification(\"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Script ‡∏û‡∏£‡πâ‡∏≠‡∏° LINE Monitoring ‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "print(\"‚úÖ Training script with monitoring created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Start Training\n",
    "send_line_notification(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• dLNk GPT V2\\n\\n‚è∞ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤\\n\\nüìä ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ\")\n",
    "\n",
    "# Run training\n",
    "%cd /content/gptprojecttrain\n",
    "!python3 train_with_monitoring.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 9: Upload to Hugging Face (After Training)\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "send_line_notification(\"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face...\")\n",
    "\n",
    "# Login to Hugging Face (need to set HF_TOKEN in Colab secrets)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token)\n",
    "    \n",
    "    # Upload model\n",
    "    api = HfApi()\n",
    "    model_path = \"./dLNk-gpt-j-6b-exploit-v2\"\n",
    "    repo_name = \"dLNk-GPT-J-6B-Exploit-V2\"\n",
    "    \n",
    "    api.create_repo(repo_name, exist_ok=True, private=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=model_path,\n",
    "        repo_id=f\"your-username/{repo_name}\",\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    \n",
    "    send_line_notification(f\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\n\\nü§ó Hugging Face: {repo_name}\")\n",
    "    print(\"‚úÖ Model uploaded to Hugging Face\")\n",
    "except Exception as e:\n",
    "    send_line_notification(f\"‚ùå ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {str(e)}\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 10: Final Summary\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Get model info\n",
    "model_dir = \"./dLNk-gpt-j-6b-exploit-v2\"\n",
    "if os.path.exists(model_dir):\n",
    "    model_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                     for dirpath, dirnames, filenames in os.walk(model_dir)\n",
    "                     for filename in filenames) / (1024**3)  # GB\n",
    "    \n",
    "    summary = f\"\"\"üéâ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\n",
    "\n",
    "üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô:\n",
    "‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•: dLNk GPT V2 Exploit Agent\n",
    "üíæ ‡∏Ç‡∏ô‡∏≤‡∏î: {model_size:.2f} GB\n",
    "üìÅ ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á: {model_dir}\n",
    "‚è∞ ‡πÄ‡∏ß‡∏•‡∏≤: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "üöÄ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\n",
    "\n",
    "üìù ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:\n",
    "1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "2. Deploy API\n",
    "3. Integrate with exploit agent\n",
    "\"\"\"\n",
    "    \n",
    "    send_line_notification(summary)\n",
    "    print(summary)\n",
    "else:\n",
    "    send_line_notification(\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à\")\n",
    "    print(\"Model directory not found\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
