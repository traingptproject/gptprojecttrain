#!/usr/bin/env python3
"""
dLNk GPT V2 Exploit Agent Training Script
Train AI agent for exploit development and execution
"""

import os
import sys
import json
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from training_config_v2_exploit import *

class ExploitAgentTrainer:
    """Trainer for exploit development AI agent"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.dataset = None
        
    def load_model_and_tokenizer(self):
        """Load base model and tokenizer"""
        print("[*] Loading model and tokenizer...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            MODEL_CONFIG["base_model"],
            trust_remote_code=True
        )
        
        # Set padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_CONFIG["base_model"],
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"[+] Model loaded: {MODEL_CONFIG['base_model']}")
        print(f"[+] Model parameters: {self.model.num_parameters():,}")
        
    def prepare_lora_model(self):
        """Prepare model with LoRA"""
        print("[*] Preparing LoRA configuration...")
        
        # Prepare model for k-bit training
        self.model = prepare_model_for_kbit_training(self.model)
        
        # Create LoRA config
        lora_config = LoraConfig(**LORA_CONFIG)
        
        # Get PEFT model
        self.model = get_peft_model(self.model, lora_config)
        
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"[+] LoRA applied")
        print(f"[+] Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
        
    def load_dataset(self):
        """Load and prepare training dataset"""
        print("[*] Loading dataset...")
        
        # Load JSONL dataset
        dataset = load_dataset(
            'json',
            data_files=DATASET_CONFIG["train_file"],
            split='train'
        )
        
        # Split into train and validation
        split_dataset = dataset.train_test_split(
            test_size=DATASET_CONFIG["validation_split"],
            seed=DATASET_CONFIG["seed"]
        )
        
        self.dataset = split_dataset
        
        print(f"[+] Dataset loaded")
        print(f"[+] Training samples: {len(self.dataset['train'])}")
        print(f"[+] Validation samples: {len(self.dataset['test'])}")
        
    def tokenize_function(self, examples):
        """Tokenize dataset examples"""
        # Format prompt
        prompts = []
        for i in range(len(examples[DATA_FORMAT["instruction_key"]])):
            instruction = examples[DATA_FORMAT["instruction_key"]][i]
            input_text = examples[DATA_FORMAT["input_key"]][i]
            output_text = examples[DATA_FORMAT["output_key"]][i]
            
            prompt = DATA_FORMAT["prompt_template"].format(
                instruction=instruction,
                input=input_text,
                output=output_text
            )
            prompts.append(prompt)
        
        # Tokenize
        tokenized = self.tokenizer(
            prompts,
            truncation=True,
            max_length=DATASET_CONFIG["max_length"],
            padding="max_length",
            return_tensors=None
        )
        
        # Set labels (same as input_ids for causal LM)
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized
    
    def prepare_dataset(self):
        """Tokenize and prepare dataset"""
        print("[*] Tokenizing dataset...")
        
        tokenized_dataset = self.dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=self.dataset["train"].column_names,
            desc="Tokenizing"
        )
        
        self.dataset = tokenized_dataset
        
        print("[+] Dataset tokenized")
        
    def train(self):
        """Train the model"""
        print("[*] Starting training...")
        
        # Training arguments
        training_args = TrainingArguments(**TRAINING_ARGS)
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Causal LM, not masked LM
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset["train"],
            eval_dataset=self.dataset["test"],
            data_collator=data_collator,
        )
        
        # Train
        trainer.train()
        
        print("[+] Training completed")
        
        # Save model
        trainer.save_model(TRAINING_ARGS["output_dir"])
        self.tokenizer.save_pretrained(TRAINING_ARGS["output_dir"])
        
        print(f"[+] Model saved to: {TRAINING_ARGS['output_dir']}")
        
    def test_model(self):
        """Test trained model with sample prompts"""
        print("[*] Testing model...")
        
        for prompt in POST_TRAINING["test_prompts"]:
            print(f"\n[*] Prompt: {prompt}")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"[+] Response: {response[:200]}...")
    
    def run_full_training(self):
        """Run complete training pipeline"""
        print("=" * 60)
        print("dLNk GPT V2 Exploit Agent Training")
        print("=" * 60)
        
        self.load_model_and_tokenizer()
        self.prepare_lora_model()
        self.load_dataset()
        self.prepare_dataset()
        self.train()
        self.test_model()
        
        print("\n[+] Training pipeline completed successfully!")

def main():
    """Main entry point"""
    trainer = ExploitAgentTrainer()
    trainer.run_full_training()

if __name__ == "__main__":
    main()
