{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ dLNk GPT - Ultimate Production Training System\n",
    "\n",
    "## ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5M+ ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
    "\n",
    "### ‚ú® ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ñ‡∏£‡∏ö‡∏Ñ‡∏£‡∏±‡∏ô:\n",
    "- ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Hugging Face Datasets (5M+ rows)\n",
    "- ‚úÖ Clone GitHub Repositories (Attack, Exploit, Hack, Bypass, Zero-day)\n",
    "- ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Exploit-DB, PayloadsAllTheThings, SecLists\n",
    "- ‚úÖ Logging ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå (‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)\n",
    "- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100 High RAM\n",
    "- ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô Overfitting ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "- ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Model ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "- ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á\n",
    "- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö Learning Rate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥\n",
    "- ‚úÖ ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ú‡πà‡∏≤‡∏ô Telegram ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå\n",
    "- ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Weights & Biases\n",
    "\n",
    "### üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:\n",
    "- **Hugging Face Datasets:** 10 datasets ‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (5M+ rows)\n",
    "- **GitHub Repos:** Attack, Exploit, Hack, Bypass, Zero-day, AI Hack\n",
    "- **Exploit-DB:** 50,000+ exploits\n",
    "- **PayloadsAllTheThings:** 10,000+ payloads\n",
    "- **SecLists:** 100,000+ fuzzing payloads\n",
    "\n",
    "### üéØ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:\n",
    "- A100 High RAM (80GB)\n",
    "- V100 (32GB)\n",
    "- T4 (16GB)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**\n",
    "- ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 3-6 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö GPU)\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î Colab tab ‡πÑ‡∏ß‡πâ‡∏ï‡∏•‡∏≠‡∏î\n",
    "- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Colab Pro ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡πÅ‡∏•‡∏∞ Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "print(\"üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU:\")\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nüìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Disk Space:\")\n",
    "total, used, free = shutil.disk_usage(\"/content\")\n",
    "print(f\"Total: {total // (2**30)} GB\")\n",
    "print(f\"Used: {used // (2**30)} GB\")\n",
    "print(f\"Free: {free // (2**30)} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ ‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages...\")\n",
    "!pip install -q transformers>=4.30.0 datasets>=2.12.0 accelerate>=0.20.0 peft>=0.4.0 bitsandbytes tensorboard wandb requests tqdm\n",
    "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Login Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # üëà ‡πÉ‡∏™‡πà token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
    "\n",
    "if not HF_TOKEN or HF_TOKEN == \"YOUR_HF_TOKEN_HERE\":\n",
    "    print(\"‚ö†Ô∏è  ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà Hugging Face token\")\n",
    "else:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Login Hugging Face ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Login Weights & Biases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "WANDB_API_KEY = \"\"  # üëà ‡πÉ‡∏™‡πà W&B API key ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏ß‡πâ)\n",
    "\n",
    "if WANDB_API_KEY:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    print(\"‚úÖ Login W&B ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ‡∏Ç‡πâ‡∏≤‡∏° W&B login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "TELEGRAM_BOT_TOKEN = \"8505747217:AAHpbWUfgrcQgJcK1JnqK9sds9nSwsTqVBg\"\n",
    "TELEGRAM_CHAT_ID = \"7420166612\"  # üëà ‡πÉ‡∏™‡πà Chat ID ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
    "\n",
    "def send_telegram(message, silent=False):\n",
    "    \"\"\"‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á Telegram ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp\"\"\"\n",
    "    if TELEGRAM_CHAT_ID == \"YOUR_REAL_CHAT_ID\":\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        full_message = f\"‚è∞ {timestamp}\\n\\n{message}\"\n",
    "        url = f\"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage\"\n",
    "        data = {\"chat_id\": TELEGRAM_CHAT_ID, \"text\": full_message, \"parse_mode\": \"HTML\"}\n",
    "        response = requests.post(url, data=data, timeout=10)\n",
    "        if not silent:\n",
    "            print(f\"üì± ‡∏™‡πà‡∏á Telegram: {message[:50]}...\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        if not silent:\n",
    "            print(f\"‚ùå ‡∏™‡πà‡∏á Telegram ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
    "        return False\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "os.environ['TELEGRAM_BOT_TOKEN'] = TELEGRAM_BOT_TOKEN\n",
    "os.environ['TELEGRAM_CHAT_ID'] = TELEGRAM_CHAT_ID\n",
    "\n",
    "if send_telegram(\"üöÄ <b>dLNk GPT Ultimate Training System</b>\\n\\n‚úÖ ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Telegram ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\\n‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô\"):\n",
    "    print(\"‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡πà‡∏á Telegram ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")\n",
    "else:\n",
    "    print(\"‚ùå ‡∏™‡πà‡∏á Telegram ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Hugging Face Datasets\n",
    "\n",
    "### üìä Top 10 Datasets (5M+ rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Hugging Face...\\n\")\n",
    "send_telegram(\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Hugging Face Datasets\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå\n",
    "os.makedirs(\"/content/hf_datasets\", exist_ok=True)\n",
    "\n",
    "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ datasets ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î\n",
    "datasets_to_download = [\n",
    "    \"jason-oneal/mitre-stix-cve-exploitdb-dataset-alpaca\",\n",
    "    \"clydeiii/cybersecurity\",\n",
    "    \"ethanolivertroy/nist-cybersecurity-training\",\n",
    "    \"Waiper/ExploitDB_DataSet\",\n",
    "    \"Trendyol/Trendyol-Cybersecurity-Instruction-Tuning-Dataset\",\n",
    "    \"Vanessasml/cybersecurity_32k_instruction_input_output\",\n",
    "    \"ystemsrx/Cybersecurity-ShareGPT-Chinese\",\n",
    "    \"Nitral-AI/Cybersecurity-ShareGPT\",\n",
    "    \"detoxioai/exploit_db_train_v1\",\n",
    "    \"darkknight25/Reverse_Shell_Payloads_Dataset\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "total_rows = 0\n",
    "\n",
    "for dataset_name in tqdm(datasets_to_download, desc=\"Downloading datasets\"):\n",
    "    try:\n",
    "        print(f\"\\nüì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î: {dataset_name}\")\n",
    "        send_telegram(f\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î: {dataset_name}\", silent=True)\n",
    "        \n",
    "        # ‡πÇ‡∏´‡∏•‡∏î dataset\n",
    "        dataset = load_dataset(dataset_name, split=\"train\")\n",
    "        \n",
    "        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤ (‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà 10,000 ‡πÅ‡∏ñ‡∏ß‡∏ï‡πà‡∏≠ dataset)\n",
    "        max_rows = min(10000, len(dataset))\n",
    "        dataset = dataset.select(range(max_rows))\n",
    "        \n",
    "        print(f\"   ‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à: {len(dataset)} rows\")\n",
    "        total_rows += len(dataset)\n",
    "        \n",
    "        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô training format\n",
    "        for item in dataset:\n",
    "            # ‡∏õ‡∏£‡∏±‡∏ö format ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ dataset\n",
    "            if \"instruction\" in item and \"output\" in item:\n",
    "                all_data.append({\n",
    "                    \"instruction\": str(item.get(\"instruction\", \"\")),\n",
    "                    \"input\": str(item.get(\"input\", \"\")),\n",
    "                    \"output\": str(item.get(\"output\", \"\"))\n",
    "                })\n",
    "            elif \"text\" in item:\n",
    "                all_data.append({\n",
    "                    \"instruction\": \"Analyze this cybersecurity content\",\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": str(item[\"text\"])[:5000]\n",
    "                })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {total_rows} rows ‡∏à‡∏≤‡∏Å {len(datasets_to_download)} datasets\")\n",
    "send_telegram(f\"‚úÖ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Hugging Face Datasets ‡πÄ‡∏™‡∏£‡πá‡∏à\\n\\nüìä ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_rows:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Clone GitHub Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á Clone GitHub Repositories...\\n\")\n",
    "send_telegram(\"üì• ‡πÄ‡∏£‡∏¥‡πà‡∏° Clone GitHub Repositories\")\n",
    "\n",
    "os.makedirs(\"/content/github_repos\", exist_ok=True)\n",
    "os.chdir(\"/content/github_repos\")\n",
    "\n",
    "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ repos ‡∏ó‡∏µ‡πà‡∏à‡∏∞ clone (‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà top repos)\n",
    "github_searches = [\n",
    "    \"exploit\",\n",
    "    \"bypass\",\n",
    "    \"payload\",\n",
    "    \"zero-day\"\n",
    "]\n",
    "\n",
    "# Clone Exploit-DB\n",
    "print(\"1Ô∏è‚É£ Clone Exploit-DB...\")\n",
    "!git clone --depth 1 --filter=blob:none --sparse https://gitlab.com/exploit-database/exploitdb.git\n",
    "%cd exploitdb\n",
    "!git sparse-checkout set exploits shellcodes\n",
    "print(\"‚úÖ Clone Exploit-DB ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\n\")\n",
    "\n",
    "# Clone PayloadsAllTheThings\n",
    "%cd /content/github_repos\n",
    "print(\"2Ô∏è‚É£ Clone PayloadsAllTheThings...\")\n",
    "!git clone --depth 1 https://github.com/swisskyrepo/PayloadsAllTheThings.git\n",
    "print(\"‚úÖ Clone PayloadsAllTheThings ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\n\")\n",
    "\n",
    "# Clone SecLists\n",
    "print(\"3Ô∏è‚É£ Clone SecLists (Fuzzing)...\")\n",
    "!git clone --depth 1 --filter=blob:none --sparse https://github.com/danielmiessler/SecLists.git\n",
    "%cd SecLists\n",
    "!git sparse-checkout set Fuzzing\n",
    "print(\"‚úÖ Clone SecLists ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\n\")\n",
    "\n",
    "%cd /content\n",
    "print(\"\\n‚úÖ Clone GitHub Repositories ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "send_telegram(\"‚úÖ Clone GitHub Repositories ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å GitHub...\\n\")\n",
    "send_telegram(\"üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å GitHub\")\n",
    "\n",
    "# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Exploit-DB\n",
    "print(\"1Ô∏è‚É£ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Exploit-DB...\")\n",
    "exploitdb_csv = \"/content/github_repos/exploitdb/files_exploits.csv\"\n",
    "if os.path.exists(exploitdb_csv):\n",
    "    with open(exploitdb_csv, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in list(reader)[:2000]:  # ‡πÄ‡∏≠‡∏≤ 2000 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
    "            if row.get('description'):\n",
    "                all_data.append({\n",
    "                    \"instruction\": f\"Explain this exploit: {row['description']}\",\n",
    "                    \"input\": f\"Platform: {row.get('platform', 'Unknown')}, Type: {row.get('type', 'Unknown')}\",\n",
    "                    \"output\": f\"This is a {row.get('type', 'unknown')} exploit for {row.get('platform', 'unknown')} platform.\"\n",
    "                })\n",
    "    print(f\"   ‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Exploit-DB ‡πÄ‡∏™‡∏£‡πá‡∏à\")\n",
    "\n",
    "# ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PayloadsAllTheThings\n",
    "print(\"\\n2Ô∏è‚É£ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• PayloadsAllTheThings...\")\n",
    "payloads_dir = \"/content/github_repos/PayloadsAllTheThings\"\n",
    "count = 0\n",
    "for md_file in Path(payloads_dir).rglob(\"*.md\"):\n",
    "    if count >= 1000:\n",
    "        break\n",
    "    try:\n",
    "        with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()[:3000]\n",
    "            if len(content) > 100:\n",
    "                all_data.append({\n",
    "                    \"instruction\": f\"Explain the payloads in: {md_file.name}\",\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": content\n",
    "                })\n",
    "                count += 1\n",
    "    except:\n",
    "        pass\n",
    "print(f\"   ‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {count} payloads ‡∏à‡∏≤‡∏Å PayloadsAllTheThings\")\n",
    "\n",
    "print(f\"\\n‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(all_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
    "send_telegram(f\"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\\n\\nüìä ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/content/training_data_ultimate.jsonl\"\n",
    "print(f\"üíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {output_file}\")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "print(f\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: {len(all_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ({file_size:.2f} MB)\")\n",
    "send_telegram(f\"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\n\\nüìä ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô: {len(all_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\\nüíæ ‡∏Ç‡∏ô‡∏≤‡∏î: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (A100 Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...\\n\")\n",
    "send_telegram(\"üöÄ <b>‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô!</b>\\n\\n‡πÇ‡∏°‡πÄ‡∏î‡∏•: GPT-J-6B\\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(all_data):,} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer\n",
    "model_name = \"EleutherAI/gpt-j-6b\"\n",
    "print(f\"üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 16 ‡πÄ‡∏õ‡πá‡∏ô 32 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A100\n",
    "    lora_alpha=64,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 32 ‡πÄ‡∏õ‡πá‡∏ô 64\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß\\n\")\n",
    "\n",
    "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° dataset\n",
    "def format_data(example):\n",
    "    text = f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "    if example['input']:\n",
    "        text += f\"### Input:\\n{example['input']}\\n\\n\"\n",
    "    text += f\"### Response:\\n{example['output']}\"\n",
    "    return tokenizer(text, truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "dataset = Dataset.from_list(all_data)\n",
    "tokenized_dataset = dataset.map(format_data, remove_columns=dataset.column_names)\n",
    "\n",
    "# Training arguments (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö A100)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/dLNk-gpt-j-6b-ultimate\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 4 ‡πÄ‡∏õ‡πá‡∏ô 8 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö A100\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\" if WANDB_API_KEY else \"none\",\n",
    "    run_name=\"dLNk-gpt-j-ultimate\",\n",
    "    logging_dir=\"/content/logs\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Custom Trainer with Telegram notifications\n",
    "class TelegramTrainer(Trainer):\n",
    "    def log(self, logs):\n",
    "        super().log(logs)\n",
    "        if \"loss\" in logs:\n",
    "            epoch = logs.get(\"epoch\", 0)\n",
    "            loss = logs.get(\"loss\", 0)\n",
    "            send_telegram(f\"üìä <b>Training Progress</b>\\n\\nEpoch: {epoch:.2f}\\nLoss: {loss:.4f}\", silent=True)\n",
    "\n",
    "trainer = TelegramTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô\n",
    "print(\"üî• ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "print(\"\\nüíæ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•...\")\n",
    "model.save_pretrained(\"/content/dLNk-gpt-j-6b-ultimate-final\")\n",
    "tokenizer.save_pretrained(\"/content/dLNk-gpt-j-6b-ultimate-final\")\n",
    "\n",
    "print(\"‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "send_telegram(\"üéâ <b>‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!</b>\\n\\n‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\\nüìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: /content/dLNk-gpt-j-6b-ultimate-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub...\")\n",
    "send_telegram(\"üì§ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub\")\n",
    "\n",
    "api = HfApi()\n",
    "repo_name = \"dLNk/gpt-j-6b-ultimate\"\n",
    "\n",
    "try:\n",
    "    api.upload_folder(\n",
    "        folder_path=\"/content/dLNk-gpt-j-6b-ultimate-final\",\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    print(f\"‚úÖ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß: https://huggingface.co/{repo_name}\")\n",
    "    send_telegram(f\"‚úÖ <b>‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!</b>\\n\\nüîó https://huggingface.co/{repo_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
    "    send_telegram(f\"‚ùå ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
